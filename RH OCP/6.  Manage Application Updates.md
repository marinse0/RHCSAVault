## Container Image Identity and Tags

### Objectives

- Relate container image tags to their identifier hashes, and identify container images from pods and containers on Kubernetes nodes.

### Kubernetes Image Tags

The full name of a container image is composed of several parts. For example, you can decompose the `registry.access.redhat.com/ubi9/nginx-120:1-86` image name into the following elements:

- The registry server is `registry.access.redhat.com`.
- The namespace is `ubi9`.
- The name is `nginx-120`. In this example, the name of the image includes the version of the software, Nginx version 1.20.
- The tag, which points to a specific version of the image, is `1-86`. If you omit the tag, then most container tools use the `latest` tag by default.

Multiple tags can refer to the same image version. The following screen capture of the Red Hat Ecosystem Catalog at https://catalog.redhat.com/software/containers/explore lists the tags for the `ubi9/nginx-120` image:

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/updates/ids/assets/nginx-120-tags.png)

In this case, the `1.86`, `latest`, and `1` tags point to the same image version. You can use any of these tags to refer to that version.

The `latest` and `1` tags are _floating tags_, because they can point to different image versions over time. For example, when developers publish a new version of the image, they change the `latest` tag to point to that new version. They also update the `1` tag to point to the latest release of that version, such as `1-87` or `1-88`.

As a user of the image, by specifying a floating tag, you ensure that you always consume the up-to-date image version that corresponds to the tag.

#### Floating Tag Issues

Vendors, organizations, and developers who publish images manage their tags and establish their own lifecycle for floating tags. They can reassign a floating tag to a new image version without notice.

As a user of the image, you might not notice that the tag that you were using now points to a different image version.

Suppose that you deploy an application on OpenShift and use the `latest` tag for the image. The following series of events might occur:

1. When OpenShift deploys the container, it pulls the image with the `latest` tag from the container registry.
2. Later, the image developer pushes a new version of the image, and reassigns the `latest` tag to that new version.
3. OpenShift relocates the pod to a different cluster node, for example because the original node fails.
4. On that new node, OpenShift pulls the image with the `latest` tag, and thereby retrieves the new image version.
5. Now the OpenShift deployment runs with a new version of the application, without your awareness of that version update.

A similar issue is that when you scale up your deployment, OpenShift starts new pods. On the nodes, OpenShift pulls the `latest` image version for these new pods. As a result, if a new version is available, then your deployment runs with containers that use different versions of the image. Application inconsistencies and unexpected behavior might occur.

To prevent these issues, select an image that is guaranteed not to change over time. You thus gain control over the lifecycle of your application: you can choose when and how OpenShift deploys a new image version.

You can select a static image version in several ways:

- Use a tag that does not change, instead of relying on floating tags.
- Use OpenShift image streams for tight control over the image versions. Another section in this course discusses image streams further.
- Use the _SHA (Secure Hash Algorithm)_ image ID instead of a tag when referencing an image version.

The distinction between a floating and non-floating tag is not a technical one, but a convention. Although it is discouraged, there is no mechanism to prevent a developer from pushing a different image to an existing tag. Thus, you must specify the SHA image ID to guarantee that the referenced container image does not change.

#### Using SHA Image ID

Developers assign tags to images. In contrast, an SHA image ID, or _digest_, is a unique identifier that the container registry computes and assigns to images. The SHA ID is an immutable string that refers to a specific image version. Using the SHA ID for identifying an image is the most secure approach.

To refer to an image by its SHA ID, replace ``name:tag`` with ``name@`SHA-ID` `` in the image name. The following example uses the SHA image ID instead of a tag.

```
registry.access.redhat.com/ubi9/nginx-120@`sha256:1be2006abd21735e7684eb4cc6eb62...`
```

To retrieve the SHA image ID from the tag, use the `oc image info` command.

---
**Note**

A multi-architecture image references images for several CPU architectures. Multi-architecture images include an index that points to the images for different platforms and CPU architectures.

---

For these images, the `oc image info` command requires you to select an architecture by using the `--filter-by-os` option:

```sh
[user@host ~]$ oc image info registry.access.redhat.com/ubi9/nginx-120:1-86
error: the image is a manifest list and contains multiple images - use --filter-by-os to select from:

  OS            DIGEST
  linux/amd64   sha256:1be2006abd21735e7684eb4cc6eb6295346a89411a187e37cd4...
  linux/arm64   sha256:d765193e823bb89b878d2d2cb8be0e0073839a6c19073a21485...
  linux/ppc64le sha256:0dd0036620f525b3ba9a46f9f1c52ac70414f939446b2ba3a07...
  linux/s390x   sha256:d8d95cc17764b82b19977bc7ef2f60ff56a3944b3c7c14071dd...
```

The following example displays the SHA ID for the image that the `1-86` tag currently points to.

```sh
[user@host ~]$ oc image info --filter-by-os linux/amd64 \ registry.access.redhat.com/ubi9/nginx-120:1-86
Name:      registry.access.redhat.com/ubi9/nginx-120:1-86
`Digest:    sha256:1be2006abd21735e7684eb4cc6eb​6295346a89411a187e37cd4a3aa2f1bd13a5`
Manifest List: sha256:5bc635dc946fedb4ba391470e8f84f9860e06a1709e30206a95ed9955...
Media Type:    application/vnd.docker.distribution.manifest.v2+json
_...output omitted..._
```

You can also use the `skopeo inspect` command. The output format differs from the `oc image info` command, although both commands report similar data.

If you use the ``oc debug node/node-name`` command to connect to a compute node, then you can list the locally available images by running the `crictl images --digests --no-trunc` command. The `--digests` option instructs the command to display the SHA image IDs, and the `--no-trunc` option instructs the command to display the full SHA string; otherwise, the command displays only the first characters.

```sh
[user@host ~]$ oc debug node/node-name
Temporary namespace openshift-debug-csn2p is created for debugging node...
Starting pod/node-name-debug ...
To use host binaries, run chroot /host
Pod IP: 192.168.50.10
If you dont see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-4.4# crictl images --digests --no-trunc \ registry.access.redhat.com/ubi9/nginx-120:1-86
IMAGE                                     TAG  DIGEST              IMAGE ID    ...
registry.access.redhat.com/ubi9/nginx-120 1-86 `sha256:1be2...13a5`  2e68...949e ...
```

The `IMAGE ID` column displays the local image identifier that the container engine assigns to the image. This identifier is not related to the SHA ID.

The container image format relies on SHA-256 hashes to identify several image components, such as the image layers or the image metadata. Because some commands also report these SHA-256 strings, ensure that you use the SHA-256 hash that corresponds to the SHA image ID. Commands often refer to the SHA image ID as the image digest.

### Selecting a Pull Policy

When you deploy an application, OpenShift selects a compute node to run the pod. On that node, OpenShift pulls the image and then starts the container.

By setting the `imagePullPolicy` attribute in the deployment resource, you can control how OpenShift pulls the image.

The following example shows the `myapp` deployment resource. The pull policy is set to `IfNotPresent`.

```sh
[user@host ~]$ oc get deployment myapp -o yaml
apiVersion: apps/v1
kind: Deployment
_...output omitted..._
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: myapp
    spec:
      containers:
      - image: registry.access.redhat.com/ubi9/nginx-120:1-86
        imagePullPolicy: IfNotPresent
        name: nginx-120
_...output omitted..._
```

The `imagePullPolicy` attribute can take the following values:

`IfNotPresent`

If the image is already on the compute node, because another container is using it or because OpenShift pulled the image during a preceding pod run, then OpenShift uses that local image. Otherwise, OpenShift pulls the image from the container registry.

If you use a floating tag in your deployment, and the image with that tag is already on the node, then OpenShift does not pull the image again, even if the floating tag might point to a newer image in the source container registry.

OpenShift sets the `imagePullPolicy` attribute to `IfNotPresent` by default when you use a tag or the SHA ID to identify the image.

`Always`

OpenShift always verifies whether an updated version of the image is available on the source container registry. To do so, OpenShift retrieves the SHA ID of the image from the registry. If a local image with that same SHA ID is already on the compute node, then OpenShift uses that image. Otherwise, OpenShift pulls the image.

If you use a floating tag in your deployment, and an image with that tag is already on the node, then OpenShift queries the registry anyway to ensure that the tag still points to the same image version. However, if the developer pushed a new version of the image and updated the floating tag, then OpenShift retrieves that new image version.

OpenShift sets the `imagePullPolicy` attribute to `Always` by default when you use the `latest` tag, or when you do not specify a tag.

`Never`

OpenShift does not pull the image, and expects the image to be already available on the node. Otherwise, the deployment fails.

To use this option, you must prepopulate your compute nodes with the images that you plan to use. You use this mechanism to improve speed or to avoid relying on a container registry for these images.

### Pruning Images from Cluster Nodes

When OpenShift deletes a pod from a compute node, it does not remove the associated image. OpenShift can reuse the images without having to pull them again from the remote registry.

Because the images consume disk space on the compute nodes, OpenShift needs to remove, or _prune_, the unused images when disk space becomes sparse. The `kubelet` process, which runs on the compute nodes, includes a garbage collector that runs every five minutes. If the usage of the file system that stores the images is above 85%, then the garbage collector removes the oldest unused images. Garbage collection stops when the file system usage drops below 80%.

The reference documentation at the end of this lecture includes instructions to adjust these default thresholds.

From a compute node, you can run the `crictl imagefsinfo` command to retrieve the name of the file system that stores the images:

```sh
[user@host ~]$ oc debug node/node-name
Temporary namespace openshift-debug-csn2p is created for debugging node...
Starting pod/_`node-name`_-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.
sh-4.4# **`chroot /host`**
sh-4.4# **`crictl imagefsinfo`**
{
  "status": {
    "timestamp": "1674465624446958511",
    "fsId": {
      `"mountpoint": "/var/lib/containers/storage/overlay-images"`
    },
    `"usedBytes"`: {
      "value": "`` `1318560` ``"
    },
    "inodesUsed": {
      "value": "446"
    }
  }
}
```

From the preceding command output, the file system that stores the images is `/var/lib/containers/storage/overlay-images`. The images consume 1318560 bytes of disk space.

From the compute node, you can use the `crictl rmi` to remove an unused image. However, pruning objects by using the `crictl` command might interfere with the garbage collector and the `kubelet` process.

It is recommended that you rely on the garbage collector to prune unused objects, images, and containers from the compute nodes. The garbage collector is configurable to better fulfill custom needs that you might have.

## Update Application Image and Settings

### Objectives

- Update applications with minimal downtime by using deployment strategies.

### Application Code, Configuration, and Data

Modern applications loosely couple code, configuration, and data. Configuration files and data are not hard-coded as part of the software. Instead, the software loads the configuration and data from an external source. This externalization enables deploying an application to different environments without requiring a change to the application source code.

OpenShift provides configuration map, secret, and volume resources to store the application configuration and data. The application code is available through container images.

Because OpenShift deploys applications from container images, developers must build a new version of the image when they update the code of their application. Organizations usually use a continuous integration and continuous delivery (CI/CD) pipeline to automatically build the image from the application source code, and then to push the resulting image to a container registry.

You use OpenShift resources, such as configuration maps and secrets, to update the configuration of the application. To control the deployment process of a new image version, you use a `Deployment` object.

### Deployment Strategies

Deploying functional application changes or new versions to users is a significant phase of the CI/CD pipelines, where you add value to the development process.

Introducing application changes carries risks, such as downtime during the deployment, bugs, or reduced application performance. You can reduce or mitigate some risks with testing and validation stages in your pipelines.

Application or service downtime can result in lost business, disruption to other services that depend on yours, and violations of service level agreements, among others. To reduce downtime and minimize risks in deployments, use a _deployment strategy_. A deployment strategy changes or upgrades an application in a way that minimizes the impact of those changes.

In OpenShift, you use `Deployment` objects to define deployments and deployment strategies. The `RollingUpdate` and the `Recreate` strategies are the main OpenShift deployment strategies.

To select the `RollingUpdate` or `Recreate` strategies, you set the `.spec.strategy.type` property of the `Deployment` object. The following snippet shows a `Deployment` object that uses the `Recreate` strategy:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
_...output omitted..._
spec:
  progressDeadlineSeconds: 600
  replicas: 10
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: myapp2
  `strategy:     type: Recreate`
  template:
_...output omitted..._
```
#### Rolling Update Strategy

The `RollingUpdate` strategy consists of updating a version of an application in stages. It replaces one instance after another until all instances are replaced.

In this strategy, both versions of the application run simultaneously, and it scales down instances of the previous version only when the new version is ready. The main drawback is that this strategy requires compatibility between the versions in the deployment.

The following graphic shows the deployment of a new version of an application by using the `RollingUpdate` strategy:

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/updates/rollout/assets/rolling-strategy.svg)

1. Some application instances run a code version that needs updating (v1). OpenShift scales up a new instance with the updated application version (v2). Because the new instance with version v2 is not ready, only the version v1 instances fulfill customer requests.
2. The instance with v2 is ready and accepts customer requests. OpenShift scales down an instance with version v1, and scales up a new instance with version v2. Both versions of the application fulfill customer requests.
3. The new instance with v2 is ready and accepts customer requests. OpenShift scales down the remaining instance with version v1.
4. No instances remain to replace. The application update was successful, and without downtime.

The `RollingUpdate` strategy supports continuous deployment, and eliminates application downtime during deployments. You can use this strategy if the different versions of your application can run at the same time.

---
**Note**

The `RollingUpdate` strategy is the default strategy if you do not specify a strategy on the `Deployment` objects.

---

The following snippet shows a `Deployment` object that uses the `RollingUpdate` strategy:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
_...output omitted..._
spec:
  progressDeadlineSeconds: 600
  replicas: 10
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: myapp2
  strategy:
    rollingUpdate:
       maxSurge: 25%
       maxUnavailable: 50%
    type: RollingUpdate
  template:
_...output omitted..._
```

Out of many parameters to configure the `RollingUpdate` strategy, the preceding snippet shows the `maxSurge` and `maxUnavailable` parameters.

During a rolling update, the number of pods for the application varies, because OpenShift starts new pods for the new revision, and removes pods from the previous revision. The `maxSurge` parameter indicates how many pods OpenShift can create above the normal number of replicas. The `maxUnavailable` parameter indicates how many pods OpenShift can remove below the normal number of replicas. You can express these parameters as percentages or as a number of pods.

If you do not configure a readiness probe for your deployment, then during a rolling update, OpenShift starts sending client traffic to new pods as soon as they are running. However, the application inside a container might not be immediately ready to accept client requests. The application might have to load files to cache, establish a network connection to a database, or perform initial tasks that might take time to complete. Consequently, OpenShift redirects client requests to a container that is not yet ready, and these requests fail.

Adding a readiness probe to your deployment prevents OpenShift from sending traffic to new pods that are not ready.

#### Recreate Strategy

In this strategy, all the instances of an application are killed first, and are then replaced with new ones. The major drawback of this strategy is that it causes a downtime in your services. For a period, no application instances are available to fulfill requests.

The following graphic shows the deployment of a new version of an application that uses the `Recreate` strategy:

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/updates/rollout/assets/recreate-strategy.svg)

1. The application has some instances that run a code version to update (v1).
2. OpenShift scales down the running instances to zero. This action causes application downtime, because no instances are available to fulfill requests.
3. OpenShift scales up new instances with a new version of the application (v2). When the new instances are booting, the downtime continues.
4. The new instances finished booting, and are ready to fulfill requests. This step is the last step of the `Recreate` strategy, and it resolves the application outage.

You can use this strategy when your application cannot have different simultaneously running code versions. You might also use it to execute data migrations or data transformations before the new code starts. This strategy is not recommended for applications that need high availability, for example, medical systems.

### Rolling out Applications

When you update a `Deployment` object, OpenShift automatically rolls out the application. If you apply several modifications in a row, such as modifying the image version, updating environment variables, and configuring the readiness probe, then OpenShift rolls out the application for each modification.

To prevent these multiple deployments, pause the rollout, apply all your modifications to the `Deployment` object, and then resume the rollout. OpenShift then performs a single rollout to apply all your modifications:

- Use the `oc rollout pause` command to pause the rollout of the `myapp` deployment:

```sh
[user@host ~]$ oc rollout pause deployment/myapp
```

- Apply all your modifications to the `Deployment` object. The following example modifies the image, an environment variable, and the readiness probe.
```sh
[user@host ~]$ oc set image deployment/myapp \ nginx-120=registry.access.redhat.com/ubi9/nginx-120:1-86
[user@host ~]$ oc set env deployment/myapp NGINX_LOG_TO_VOLUME=1
[user@host ~]$ oc set probe deployment/myapp --readiness --get-url http://:8080
```

- Resume the rollout:
```sh
[user@host ~]$ oc rollout resume deployment/myapp
```

OpenShift rolls out the application to apply all your modifications to the `Deployment` object.


You can follow a similar process when you create and configure a new deployment:

- Create the deployment, and set the number of replicas to zero. This way, OpenShift does not roll out your application, and no pods are running.
```sh
[user@host ~]$ oc create deployment myapp2 \ --image registry.access.redhat.com/ubi9/nginx-120:1-86 --replicas 0
[user@host ~]$ oc get deployment/myapp2
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
myapp2   `0/0     0            0`           9s
```

- Apply the configuration to the `Deployment` object. The following example adds a readiness probe.
```sh
[user@host ~]$ oc set probe deployment/myapp2 --readiness --get-url http://:8080
```

- Scale up the deployment. OpenShift rolls out the application.
```sh
[user@host ~]$ oc scale deployment/myapp2 --replicas 10
[user@host ~]$ oc get deployment/myapp2
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
myapp2   `10/10   10           10`          18s
```

#### Monitoring Replica Sets

Whenever OpenShift rolls out an application from a `Deployment` object, it creates a `ReplicaSet` object. Replica sets are responsible for creating and monitoring the pods. If a pod fails, then the `ReplicaSet` object deploys a new one.

To deploy pods, replica sets use the pod template definition from the `Deployment` object. OpenShift copies the template definition from the `Deployment` object when it creates the `ReplicaSet` object.

When you update the `Deployment` object, OpenShift does not update the existing `ReplicaSet` object. Instead, it creates another `ReplicaSet` object with the new pod template definition. Then, OpenShift rolls out the application according to the update strategy.

Thus, several `ReplicaSet` objects for a deployment can exist at the same time on your system. During a rolling update, the old and the new `ReplicaSet` objects coexist and coordinate the rollout of the new application version. After the rollout completes, OpenShift keeps the old `ReplicaSet` object so that you can roll back if the new application version does not operate correctly.

The following graphic shows a `Deployment` object and two `ReplicaSet` objects. The old `ReplicaSet` object for version 1 of the application does not run any pods. The current `ReplicaSet` object for version 2 of the application manages three replicated pods.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/updates/rollout/assets/replicaset.svg)

Do not directly change or delete `ReplicaSet` objects, because OpenShift manages them through the associated `Deployment` objects. The `.spec.revisionHistoryLimit` attribute in `Deployment` objects specifies how many `ReplicaSet` objects OpenShift keeps. OpenShift automatically deletes the extra `ReplicaSet` objects. Also, when you delete a `Deployment` object, OpenShift deletes all the associated `ReplicaSet` objects.

Run the `oc get replicaset` command to list the `ReplicaSet` objects. OpenShift uses the `Deployment` object name as a prefix for the `ReplicaSet` objects.

```sh
[user@host ~]$ oc get replicaset
NAME                DESIRED   CURRENT   READY   AGE
myapp2-574968dd59   0         0         0       3m27s
myapp2-76679885b9   10        10        10      22s
myapp2-786cbf9bc8   0         0         0       114s
```

The preceding output shows three `ReplicaSet` objects for the `myapp2` deployment. Whenever you modified the `myapp2` deployment, OpenShift created a `ReplicaSet` object. The second object in the list is active and monitors 10 pods. The other `ReplicaSet` objects do not manage any pods. They represent the previous versions of the `Deployment` object.

During a rolling update, two `ReplicaSet` objects are active. The old `ReplicaSet` object is scaling down, and at the same time the new object is scaling up:

```sh
[user@host ~]$ oc get replicaset
NAME                DESIRED   CURRENT   READY   AGE
myapp2-574968dd59   0         0         0       13m
myapp2-5fb5766df5   4         4         2       21s  # 1
myapp2-76679885b9   8         8         8       10m  # 2
myapp2-786cbf9bc8   0         0         0       11m
```

|     |                                                                                                                                                                                                       |
| --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1   | The new `ReplicaSet` object already started four pods, but the `READY` column shows that the readiness probe succeeded for only two pods so far. These two pods are likely to receive client traffic. |
| 2   | The `ReplicaSet` object already scaled down from 10 to 8 pods.                                                                                                                                        |

#### Managing Rollout

Because OpenShift preserves `ReplicaSet` objects from earlier deployment versions, you can roll back if you notice that the new version of the application does not work.

Use the `oc rollout undo` command to roll back to the preceding deployment version. The command uses the existing `ReplicaSet` object for that version to roll back the pods. The command also reverts the `Deployment` object to the preceding version.

```sh
[user@host ~]$ oc rollout undo deployment/myapp2
```
Use the `oc rollout status` command to control the rollout process:

```sh
[user@host ~]$ oc rollout status deployment/myapp2
deployment "myapp2" successfully rolled out
```

If the rollout operation fails, because you specify a wrong container image name or the readiness probe fails, then OpenShift does not automatically roll back your deployment. In this case, run the `oc rollout undo` command to revert to the preceding working configuration.

By default, the `oc rollout undo` command rolls back to the preceding deployment version. If you need to roll back to an earlier revision, then list the available revisions and add the ``--to-revision _`rev`_`` option to the `oc rollout undo` command.

- Use the `oc rollout history` command to list the available revisions:
```sh
[user@host ~]$ oc rollout history deployment/myapp2
deployment.apps/myapp2
REVISION  CHANGE-CAUSE
1         <none>
3         <none>
4         <none>
5         <none>
```

### Note

The `CHANGE-CAUSE` column provides a user-defined message that describes the revision. You can store the message in the `kubernetes.io/change-cause` deployment annotation after every rollout:

```sh
[user@host ~]$ oc annotate deployment/myapp2 \
kubernetes.io/change-cause="Image updated to 1-86"
deployment.apps/myapp2 annotated
[user@host ~]$ oc rollout history deployment/myapp2
deployment.apps/myapp2
REVISION  CHANGE-CAUSE
1         <none>
3         <none>
4         <none>
5         Image updated to 1-86
```

- Add the `--revision` option to the `oc rollout history` command for more details about a specific revision:
```sh
[user@host ~]$ oc rollout history deployment/myapp2 --revision 1
deployment.apps/myapp2 with revision #1
Pod Template:
  Labels: app=myapp2
    pod-template-hash=574968dd59
  Containers:
    nginx-120:
      Image:       registry.access.redhat.com/ubi9/nginx-120:1-86
      Port:        <none>
      Host Port:   <none>
      Environment: <none>
      Mounts:      <none>
   Volumes: <none>
```

The `pod-template-hash` attribute is the suffix of the associated `ReplicaSet` object. You can inspect that `ReplicaSet` object for more details by using the `oc describe replicaset myapp2-574968dd59` command, for example.

- Roll back to a specific revision by adding the `--to-revision` option to the `oc rollout undo` command:
```sh
[user@host ~]$ oc rollout undo deployment/myapp2 --to-revision 1
```

If you use floating tags to refer to container image versions in deployments, then the resulting image when you roll back a deployment might have changed in the container registry. Thus, the image that you run after the rollback might not be the original one that you used.

To prevent this issue, use OpenShift image streams for referencing images instead of floating tags. Another section in this course discusses image streams further.