## Deploy Applications from Images and Templates

### Objectives

- Identify the main resources and settings that Kubernetes uses to manage long-lived applications and demonstrate how OpenShift simplifies common application deployment workflows.
    

### Deploying Applications

Microservices and DevOps are growing trends in enterprise software. Containers and Kubernetes gained popularity alongside those trends, but have become categories of their own. Container-based infrastructures support most types of traditional and modern applications.

The term _application_ can refer to your software system or to a service within it. Given this ambiguity, it is clearer to refer directly to resources, services, and other components.

### Resources and Resource Definitions

Kubernetes manages applications, or services, as a loose collection of resources. _Resources_ are configuration pieces for components in the cluster. When you create a resource, the corresponding component is not created immediately. Instead, the cluster is responsible for eventually creating the component.

A _resource type_ represents a specific component type, such as a pod. Kubernetes ships with many default resource types, some of which overlap in function. Red Hat OpenShift Container Platform (RHOCP) includes the default Kubernetes resource types, and provides other resource types of its own. To add resource types, you can create or import custom resource definitions (CRDs).

### Managing Resources

You can add, view, and edit resources in various formats, including YAML and JSON. Traditionally, YAML is the most common format.

You can delete resources in batch by using label selectors or by deleting the entire project or namespace. For example, the following command deletes only deployments with the `app=my-app` label.

[user@host ~]$ **`oc delete deployment -l app=my-app`**
_...output omitted..._

Similar to creation, deleting a resource is not immediate, but is instead a request for eventual deletion.

**Note

Commands that are executed without specifying a namespace are executed in the user's current namespace.

### Common Resource Types and Their Uses

The following resources are standard Kubernetes resources unless otherwise noted.

#### Templates

Similar to projects, templates are an RHOCP addition to Kubernetes. A _template_ is a YAML manifest that contains parameterized definitions of one or more resources. RHOCP provides predefined templates in the `openshift` namespace.

Process a template into a list of resources by using the `oc process` command, which replaces values and generates resource definitions. The resulting resource definitions create or update resources in the cluster by supplying them to the `oc apply` command.

For example, the following command processes a `mysql-template.yaml` template file and generates four resource definitions.

[user@host ~]$ **`oc process -f mysql-template.yaml -o yaml`**
apiVersion: v1
items:
- apiVersion: v1
  kind: Secret
  _...output omitted..._
- apiVersion: v1
  kind: Service
  _...output omitted..._
- apiVersion: v1
  kind: PersistentVolumeClaim
  _...output omitted..._
- apiVersion: apps.openshift.io/v1
  kind: DeploymentConfig
  _...output omitted..._

The `--parameters` option instead displays the parameters of a template. For example, the following command lists the parameters of the `mysql-template.yaml` file.

[user@host ~]$ **`oc process -f mysql-template.yaml --parameters`**
NAME                    DESCRIPTION _...output omitted..._
_...output omitted..._
MYSQL_USER              Username for MySQL user   _...output omitted..._
MYSQL_PASSWORD          Password for the MySQL connection    _...output omitted..._
MYSQL_ROOT_PASSWORD     Password for the MySQL root user.    _...output omitted..._
MYSQL_DATABASE          Name of the MySQL database accessed. _...output omitted..._
VOLUME_CAPACITY         Volume space available for data,     _...output omitted..._

You can use templates with the `new-app` command from RHOCP. In the following example, the `new-app` command uses the `mysql-persistent` template to create a MySQL application and its supporting resources.

[user@host ~]$ **`oc new-app --template mysql-persistent`**
--> Deploying template "db-app/mysql-persistent" to project db-app
_...output omitted..._
     The following service(s) have been created in your project: mysql.

            Username: userQSL
            Password: pyf0yElPvFWYQQou
       Database Name: sampledb
      Connection URL: mysql://mysql:3306/
_...output omitted..._
     * With parameters:
        * Memory Limit=512Mi
        * Namespace=openshift
        * Database Service Name=mysql
        * MySQL Connection Username=userQSL # generated
        * MySQL Connection Password=pyf0yElPvFWYQQou # generated
        * MySQL root user Password=HHbdurqWO5gAog2m # generated
        * MySQL Database Name=sampledb
        * Volume Capacity=1Gi
        * Version of MySQL Image=8.0-el8

--> Creating resources ... ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
    secret "mysql" created
    service "mysql" created
    persistentvolumeclaim "mysql" created
    deploymentconfig.apps.openshift.io "mysql" created
--> Success
_...output omitted..._

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_templates-CO3-1)|Notice that several resources are created to meet the requirements of the deployment, including a secret, a service, and a persistent volume claim.|

**Note

You can specify environment variables to be configured in creating your application.

You can also use templates when creating applications from the web console by using the Developer Catalog. From the Developer perspective, navigate to the +Add menu and click All Services in the Developer Catalog section to open the catalog. Then, enter the application name in the filter to search for a template for your application. You can instantiate the template and change the default values, such as the Git repository, the memory limits, the application version, and much more.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/newapp/assets/templatewebconsole.png)|

You can add an application based on a template by changing to the Developer perspective, and then moving to the Topology menu and clicking Start building application or clicking the book icon.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/newapp/assets/topologymenu.png)|

#### Pod

From the RHOCP documentation, a _pod_ is defined as "the smallest compute unit that can be defined, deployed, and managed". A pod runs one or more containers that represent a single application. Containers in the pod share resources, such as networking and storage.

The following example shows a definition of a pod:

apiVersion: v1
kind: Pod ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata: ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
  annotations: { ... }
  labels:
    deployment: docker-registry-1
    deploymentconfig: docker-registry
  name: registry
  namespace: pod-registries
spec: ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
  containers:
  - env:
    - name: OPENSHIFT_CA_DATA
      value: ...
    image: openshift/origin-docker-registry:v0.6.2
    imagePullPolicy: IfNotPresent
    name: registry
    ports:
    - containerPort: 5000
      protocol: TCP
    resources: {}
    securityContext: { ... }
    volumeMounts:
    - mountPath: /registry
      name: registry-storage
  dnsPolicy: ClusterFirst
  imagePullSecrets:
  - name: default-dockercfg-at06w
  restartPolicy: Always
  serviceAccount: default
  volumes:
  - emptyDir: {}
    name: registry-storage
status: ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
  conditions: { ... }

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_pod-CO4-1)|Resource kind set to `Pod`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_pod-CO4-2)|Information that describes your application, such as the name, project, attached labels, and annotations.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_pod-CO4-3)|Section where the application requirements are specified, such as the container name, the container image, environment variables, volume mounts, network configuration, and volumes.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_pod-CO4-4)|Indicates the last condition of the pod, such as the last probe time, the last transition time, the status setting as `true` or `false`, and more.|

#### Deployment

Similar to deployment configurations, _deployments_ define the intended state of a replica set. _Replica sets_ maintain a configurable number of pods that match a specification.

Replica sets are generally similar to and a successor to replication controllers. This difference in intermediate resources is the primary difference between deployments and deployment configurations.

Deployments and replica sets are a Kubernetes-standard replacement for deployment configurations and replication controllers, respectively. Use a deployment unless you need a feature that is specific to deployment configurations.

The following example shows the definition of a deployment:

apiVersion: apps/v1
kind: Deployment ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata:
  name: hello-openshift ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
spec:
  replicas: 1 ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
  selector:
    matchLabels:
      app: hello-openshift
  template: ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
    metadata:
      labels:
        app: hello-openshift
    spec:
      containers:
      - name: hello-openshift ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
        image: openshift/hello-openshift:latest ![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)
        ports: ![7](https://rol.redhat.com/rol/static/roc/Common_Content/images/7.svg)
        - containerPort: 80

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-1)|Resource kind set to `Deployment`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-2)|Name of the deployment resource.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-3)|Number of running instances.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-4)|Section to define the metadata, labels, and the container information of the deployment resource.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-5)|Name of the container.|
|[![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-6)|Resource of the image to use for creating the deployment resource.|
|[![7](https://rol.redhat.com/rol/static/roc/Common_Content/images/7.svg)](https://rol.redhat.com/rol/app/#_deployment-CO5-7)|Port configuration, such as the port number, name of the port, and the protocol.|

#### Deployment Configurations

_Deployment configurations_ define the specification of a pod. They manage pods by creating _replication controllers_, which manage the number of replicas of a pod. Deployment configurations and replication controllers are an RHOCP addition to Kubernetes.

**Note

As of OpenShift Container Platform 4.14, deployment configuration objects are deprecated. Deployment configurations objects are still supported, but are not recommended for new installations.

Instead, use Deployment objects or another alternative to provide declarative updates for pods.

#### Projects

RHOCP adds projects to enhance the function of Kubernetes namespaces. A _project_ is a Kubernetes namespace with additional annotations, and is the primary method for managing access to resources for regular users. Projects can be created from templates and must use Role Based Access Control (RBAC) for organization and permission management. Administrators must grant cluster users access to a project. If a cluster user is allowed to create projects, then the user automatically has access to their created projects.

Projects provide logical and organizational isolation to separate your application component resources. Resources in one project can access resources in other projects, but not by default.

The following example shows the definition of a project:

apiVersion: project.openshift.io/v1
kind: Project ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata:
  name: test ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
spec:
  finalizers: ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
  - kubernetes

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_projects-CO6-1)|Resource kind set to `Project`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_projects-CO6-2)|Name of the project.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_projects-CO6-3)|A finalizer is a special metadata key that tells Kubernetes to wait until a specific condition is met before it fully deletes a resource.|

#### Services

You can configure internal pod-to-pod network communication in RHOCP by using the _Service_ object. Applications send requests to the service name and port. RHOCP provides a virtual network, which reroutes such requests to the pods that the service targets by using labels.

The following example shows the definition of a service:

apiVersion: v1
kind: Service ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata:
  name: docker-registry ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
  namespace: test ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
spec:
  selector:
    app: MyApp ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
  ports:
  - protocol: TCP ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
    port: 80 ![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)
    targetPort: 9376 ![7](https://rol.redhat.com/rol/static/roc/Common_Content/images/7.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_services-CO7-1)|Resource kind set to `Service`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_services-CO7-2)|Name of the service.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_services-CO7-3)|Project name where the service resource exists.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_services-CO7-4)|The label selector identifies all pods with the attached `app=MyApp` label and adds the pods to the service endpoints.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_services-CO7-5)|Internet protocol set to `TCP`.|
|[![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)](https://rol.redhat.com/rol/app/#_services-CO7-6)|Port that the service listens on.|
|[![7](https://rol.redhat.com/rol/static/roc/Common_Content/images/7.svg)](https://rol.redhat.com/rol/app/#_services-CO7-7)|Port on the backing pods, which the service forwards connections to.|

#### Persistent Volume Claims

RHOCP uses the Kubernetes persistent volume (PV) framework to enable cluster administrators to provision persistent storage for a cluster. Developers can use persistent volume claims (PVCs) to request PV resources without having specific knowledge of the underlying storage infrastructure. After a PV is bound to a PVC, that PV cannot then be bound to additional PVCs. Because PVCs are namespaced objects and PVs are globally scoped objects, this binding effectively scopes a bound PV to a single namespace until the binding PVC is deleted.

The following example shows the definition of a persistent volume claim:

apiVersion: v1
kind: PersistentVolumeClaim ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata:
  name: mysql-pvc ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
spec:
  accessModes:
    - ReadWriteOnce ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
  resources:
    requests:
      storage: 1Gi ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
  storageClassName: nfs-storage ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
status:
  ...

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_persistent_volume_claims-CO8-1)|Resource kind set to `PersistentVolumeClaim`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_persistent_volume_claims-CO8-2)|Name of the service.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_persistent_volume_claims-CO8-3)|The access mode, to define the read/write and mount permissions.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_persistent_volume_claims-CO8-4)|The amount of storage that is available to the PVC.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_persistent_volume_claims-CO8-5)|Name of the StorageClass that the claim requires.|

#### Secrets

Secrets provide a mechanism to hold sensitive information, such as passwords, private source repository credentials, sensitive configuration files, and credentials to an external resource, such as an SSH key or OAuth token. You can mount secrets into containers by using a volume plug-in. Kubernetes can also use secrets to perform actions, such as declaring environment variables, on a pod. Secrets can store any type of data. Kubernetes and OpenShift support different types of secrets, such as service account tokens, SSH keys, and TLS certificates.

The following example shows the definition of a secret:

apiVersion: v1
kind: Secret ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
metadata:
  name: example-secret ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
  namespace: my-app ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
type: Opaque ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
data: ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
  username: bXl1c2VyCg==
  password: bXlQQDU1Cg==
stringData: ![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)
  hostname: myapp.mydomain.com
  secret.properties: |
    property1=valueA
    property2=valueB

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-1)|Resource kind set to `Secret`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-2)|Name of the service.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-3)|Project name where the service resource exists.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-4)|Specifies the type of secret.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-5)|Specifies the encoded string and data.|
|[![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)](https://rol.redhat.com/rol/app/#_secrets-CO9-6)|Specifies the decoded string and data.|

### Managing Resources from the Command Line

Kubernetes and RHOCP have many commands to create and modify cluster resources. Some commands are part of core Kubernetes, whereas others are exclusive additions to RHOCP.

The resource management commands generally fall into one of two categories: declarative or imperative. An _imperative_ command instructs what the cluster does. A _declarative_ command defines the state that the cluster attempts to match.

#### Imperative Resource Management

The `create` command is an imperative way to create resources, and is included in both of the `oc` and `kubectl` commands. For example, the following command creates a deployment called `my-app` that creates pods that are based on the specified image.

[user@host ~]$ **`oc create deployment my-app --image example.com/my-image:dev`**
deployment.apps/my-app created

Use the `set` command to define attributes on a resource, such as environment variables. For example, the following command adds the `TEAM=red` environment variable to the preceding deployment.

[user@host ~]$ **`oc set env deployment/my-app TEAM=red`**
deployment.apps/my-app updated

Another imperative approach to creating a resource is the `run` command. In the following example, the `run` command creates the `example-pod` pod.

[user@host ~]$ **`oc run example-pod \`** ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
**`--image=registry.access.redhat.com/ubi8/httpd-24 \`** ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
**`--env GREETING='Hello from the awesome container' \`** ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
**`--port 8080`** ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
pod/example-pod created

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_imperative_resource_management-CO10-1)|The pod `.metadata.name` definition.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_imperative_resource_management-CO10-2)|The image for the single container in this pod.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_imperative_resource_management-CO10-3)|The environment variable for the single container in this pod.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_imperative_resource_management-CO10-4)|The port metadata definition.|

The imperative commands are a faster way of creating pods, because such commands do not require a pod object definition. However, developers cannot handle versioning or incrementally change the pod definition.

Generally, developers test a deployment by using imperative commands, and then use the imperative commands to generate the pod object definition. Use the `--dry-run=client` option to avoid creating the object in RHOCP. Additionally, use the `-o yaml` or `-o json` option to configure the definition format.

The following command is an example of generating the YAML definition for the `example-pod` pod:

[user@host ~]$ **`oc run example-pod \ --image=registry.access.redhat.com/ubi8/httpd-24 \ --env GREETING='Hello from the awesome container' \ --port 8080 \ --dry-run=client -o yaml`**
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: example-pod
  name: example-pod
spec:
  containers:
_...output omitted..._

Managing resources in this way is imperative, because you are instructing the cluster what to do rather than declaring the intended outcomes.

#### Declarative Resource Management

Using the `create` command does not guarantee that you are creating resources imperatively. For example, providing a manifest declaratively creates the resources that are defined in the YAML file, such as in the following command.

[user@host ~]$ **`oc create -f my-app-deployment.yaml`**
deployment.apps/my-app created

RHOCP adds the `new-app` command, which provides another declarative way to create resources. This command uses heuristics to automatically determine which types of resources to create based on the specified parameters. For example, the following command deploys the `my-app` application by creating several resources, including a deployment resource, from a YAML manifest file.

[user@host ~]$ **`oc new-app --file=./example/my-app.yaml`**
_...output omitted..._
--> Creating resources ...
    imagestream.image.openshift.io "my-app" created
    `deployment.apps "my-app" created`
    services "my-app" created
_...output omitted..._

In both of the preceding `create` and `new-app` examples, you are _declaring_ the intended state of the resources, and so they are declarative.

You can also use the `new-app` command with templates for resource management. A template describes the intended state of resources that must be created for an application to run, such as deployment configurations and services. Supplying a template to the `new-app` command is a form of declarative resource management.

The `new-app` command also includes options, such as the `--param` option, that customize an application deployment declaratively. For example, when the `new-app` is used with a template, you can include the `--param` option to override a parameter value in the template.

[user@host ~]$ **`oc new-app --template mysql-persistent \ --param MYSQL_USER=operator --param MYSQL_PASSWORD=myP@55 \ --param MYSQL_DATABASE=mydata \ --param DATABASE_SERVICE_NAME=db`**
--> Deploying template "db-app/mysql-persistent" to project db-app
_...output omitted..._
     The following service(s) have been created in your project: db.

            Username: operator
            Password: myP@55
       Database Name: mydata
      Connection URL: mysql://db:3306/
_...output omitted..._
     * With parameters:
        * Memory Limit=512Mi
        * Namespace=openshift
        * Database Service Name=db
        * MySQL Connection Username=operator
        * MySQL Connection Password=myP@55
        * MySQL root user Password=tlH8BThuVgnIrCon # generated
        * MySQL Database Name=mydata
        * Volume Capacity=1Gi
        * Version of MySQL Image=8.0-el8

--> Creating resources ...
    secret "db" created
    service "db" created
    persistentvolumeclaim "db" created
    deploymentconfig.apps.openshift.io "db" created
--> Success
_...output omitted..._

Similar to the `create` command, you can use the `new-app` command imperatively. When using a container image with the `new-app`, you are instructing the cluster what to do, rather than declaring the intended outcomes. For example, the following command deploys the `example.com/my-app:dev` image by creating several resources, including a deployment resource.

[user@host ~]$ **`oc new-app --image example.com/my-app:dev`**
_...output omitted..._
--> Creating resources ...
    imagestream.image.openshift.io "my-app" created
    `deployment.apps "my-app" created`
_...output omitted..._

You can also supply a Git repository to the `new-app` command. The following command creates an application named `httpd24` by using a Git repository.

[user@host ~]$ **`oc new-app https://github.com/apache/httpd.git#2.4.56`**
_...output omitted..._
--> Creating resources ...
    imagestream.image.openshift.io "httpd24" created
    `deployment.apps "httpd24" created`
_...output omitted..._

### Retrieving Resource Information

You can supply an `all` argument to commands, to specify a list of common resource types. However, the `all` option does not include every available resource type. Instead, `all` is a shorthand form for a predefined subset of types. When you use this command argument, ensure that `all` includes any intended types to address.

You can view detailed information about a resource, such as the defined parameters, by using the `describe` command. For example, RHOCP provides templates in the `openshift` project to use with the `oc new-app` command. The following example command displays detailed information about the `mysql-ephemeral` template:

[user@host ~]$ **`oc describe template mysql-ephemeral -n openshift`**
Name:		mysql-ephemeral
Namespace:	openshift
_...output omitted..._
Parameters:
    Name:		MEMORY_LIMIT
    Display Name:	Memory Limit
    Description:	Maximum amount of memory the container can use.
    Required:		true
    Value:		512Mi

    Name:		NAMESPACE
    Display Name:	Namespace
    Description:	The OpenShift Namespace where the ImageStream resides.
    Required:		false
    Value:		openshift
_...output omitted..._
Objects:
    Secret				                         ${DATABASE_SERVICE_NAME}
    Service				                        ${DATABASE_SERVICE_NAME}
    DeploymentConfig.apps.openshift.io	${DATABASE_SERVICE_NAME}

The `describe` command cannot generate structured output, such as the YAML or JSON formats. Without a structured format, the `describe` command cannot parse or filter the output with tools such as JSONPath or Go templates. Instead, use the `get` command to generate and then to parse the structured output of a resource.

## Manage Long-lived and Short-lived Applications by Using the Kubernetes Workload API

### Objectives

- Deploy containerized applications as pods that Kubernetes workload resources manage.
    

### Kubernetes Workload Resources

The Kubernetes Workloads API consists of resources that represent various types of cluster tasks, jobs, and workloads. This API is composed of various Kubernetes APIs and extensions that simplify deploying and managing applications. These resource types of the Workloads API are most often used:

- Jobs
    
- Deployments
    
- Stateful sets
    

#### Jobs

A _job_ resource represents a one-time task to perform on the cluster. As with most cluster tasks, jobs are executed via pods. If a job's pod fails, then the cluster retries a number of times that the job specifies. The job does not run again after a specified number of successful completions.

Jobs differ from using the `kubectl run` and `oc run` commands; both of the latter create only a pod.

Common uses for jobs include the following tasks:

- Initializing or migrating a database
    
- Calculating one-off metrics from information within the cluster
    
- Creating or restoring from a data backup
    

The following example command creates a job that logs the date and time:

[user@host ~]$ **`oc create job \`** ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
**`date-job \`** ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
**`--image registry.access.redhat.com/ubi8/ubi \`** ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
**`-- /bin/bash -c "date"`** ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_jobs-CO11-1)|Creates a job resource.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_jobs-CO11-2)|Specifies a job name of `date-job`.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_jobs-CO11-3)|Sets `registry.access.redhat.com/ubi8/ubi` as the container image for the job pods.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_jobs-CO11-4)|Specifies the command to run within the pods.|

You can also create a job from the web console by clicking the Workloads → Jobs menu. Click Create Job and customize the YAML manifest.

#### Cron Jobs

A _cron job_ resource builds on a regular job resource by enabling you to specify how often the job should run. Cron jobs are useful for creating periodic and recurring tasks, such as backups or report generation. Cron jobs can also schedule individual tasks for a specific time, such as to schedule a job for a low activity period. Similar to the `crontab` (cron table) file on a Linux system, the `CronJob` resource uses the `Cron` format for scheduling. A `CronJob` resource creates a `job` resource based on the configured time zone on the control plane node that runs the cron job controller.

The following example command creates a cron job named `date` that prints the system date and time every minute:

[user@host ~]$ **`oc create cronjob date-cronjob \`** ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
**`--image registry.access.redhat.com/ubi8/ubi \`** ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
**`--schedule "*/1 * * * *" \`** ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
**`-- date`** ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_cron_jobs-CO12-1)|Creates a cron job resource with a name of `date-cronjob`.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_cron_jobs-CO12-2)|Sets the `registry.access.redhat.com/ubi8/ubi` as the container image for the job pods.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_cron_jobs-CO12-3)|Specifies the schedule for the job in `Cron` format.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_cron_jobs-CO12-4)|The command to execute within the pods.|

You can also create a cronjob from the web console by clicking the Workloads → CronJobs menu. Click Create CronJob and customize the YAML manifest.

#### Deployments

A _deployment_ creates a replica set to maintain pods. A _replica set_ maintains a specified number of replicas of a pod. Replica sets use selectors, such as a label, to identify pods that are part of the set. Pods are created or removed until the replicas reach the number that the deployment specifies. Replica sets are not managed directly. Instead, deployments indirectly manage replica sets.

Unlike a job, a deployment's pods are re-created after crashing or deletion. The reason is that deployments use replica sets.

The following example command creates a deployment:

[user@host ~]$ **`oc create deployment \`** ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
**`my-deployment \`** ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
**`--image registry.access.redhat.com/ubi8/ubi \`** ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
**`--replicas 3`** ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_deployments-CO13-1)|Creates a deployment resource.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_deployments-CO13-2)|Specifies `my-deployment` as the deployment name.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_deployments-CO13-3)|Sets `registry.access.redhat.com/ubi8/ubi` as the container image for the pods.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_deployments-CO13-4)|Sets the deployment to maintain three instances of the pod.|

You can also create a deployment from the web console by clicking the Deployments tab in the Workloads menu.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/workloads/assets/deployments.png)|

Click Create Deployment and customize the form or the YAML manifest.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/workloads/assets/formdeployment.png)|

Pods in a replica set are identical and match the pod template in the replica set definition. If the number of replicas is not met, then a new pod is created by using the template. For example, if a pod crashes or is otherwise deleted, then a new one is created to replace it.

_Labels_ are a type of resource metadata that are represented as string key-value pairs. A label indicates a common trait for resources with that label. For example, you might attach a `layer=frontend` label to resources that relate to an application's UI components.

Many `oc` and `kubectl` commands accept a label to filter affected resources. For example, the following command deletes all pods with the `environment=testing` label:

[user@host ~]$ **`oc delete pod -l environment=testing`**

By liberally applying labels to resources, you can cross-reference resources and craft precise selectors. A _selector_ is a query object that describes the attributes of matching resources.

Certain resources use selectors to find other resources. In the following YAML excerpt, an example replica set uses a selector to match its pods.

apiVersion: apps/v1
kind: ReplicaSet
_...output omitted..._
spec:
  replicas: 1
  `selector:     matchLabels:       app: httpd       pod-template-hash: 7c84fbdb57`
_...output omitted..._

#### Stateful Sets

Like deployments, _stateful sets_ manage a set of pods based on a container specification. However, each pod that a stateful set creates is unique. Pod uniqueness is useful when, for example, a pod needs a unique network identifier or persistent storage.

As their name implies, stateful sets are for pods that require state within the cluster. Deployments are used for stateless pods.

Stateful sets are covered further in a later chapter.

## Kubernetes Pod and Service Networks

### Objectives

- Interconnect applications pods inside the same cluster by using Kubernetes services.
    

### The Software-defined Network

Kubernetes implements software-defined networking (SDN) to manage the network infrastructure of the cluster and user applications. The SDN is a virtual network that encompasses all cluster nodes. The virtual network enables communication between any container or pod inside the cluster. Cluster node processes that Kubernetes pods manage can access the SDN. However, the SDN is not accessible from outside the cluster, nor to regular processes on cluster nodes. With the software-defined networking model, you can manage network services through the abstraction of several networking layers.

With the SDN, you can manage the network traffic and network resources programmatically, so that the organization teams can decide how to expose their applications. The SDN implementation creates a model that is compatible with traditional networking practices. It makes pods akin to virtual machines in terms of port allocation, IP address leasing, and reservation.

With the SDN design, you do not need to change how application components communicate with each other, which helps to containerize legacy applications. If your application is composed of many services that communicate over the TCP/UDP stack, then this approach still works, because containers in a pod use the same network stack.

The following diagram shows how all pods are connected to a shared network:

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-pod-network.svg)|

Figure 4.5: How the Kubernetes SDN manages the network

Among the many features of SDN, with open standards, vendors can propose their solutions for centralized management, dynamic routing, and tenant isolation.

### Kubernetes Networking

Networking in Kubernetes provides a scalable means of communication between containers.

Kubernetes networking provides the following capabilities:

- Highly coupled container-to-container communications
    
- Pod-to-pod communications
    
- Pod-to-service communications
    
- External-to-service communication: covered in [the section called “ Scale and Expose Applications to External Access ”](https://rol.redhat.com/rol/app/courses/do180-4.14/pages/ch04s07)
    

Kubernetes automatically assigns an IP address to every pod. However, pod IP addresses are unstable, because pods are ephemeral. Pods are constantly created and destroyed across the nodes in the cluster. For example, when you deploy a new version of your application, Kubernetes destroys the existing pods and then deploys new ones.

All containers within a pod share networking resources. The IP address and MAC address that are assigned to the pod are shared among all containers in the pod. Thus, all containers within a pod can reach each other's ports through the loopback address, `localhost`. Ports that are bound to localhost are available to all containers that run within the pod, but never to containers outside it.

By default, the pods can communicate with each other even if they run on different cluster nodes or belong to different Kubernetes namespaces. Every pod is assigned an IP address in a flat shared networking namespace that has full communication with other physical computers and containers across the network. All pods are assigned a unique IP address from a Classless Inter-Domain Routing (CIDR) range of host addresses. The shared address range places all pods in the same subnet.

Because all the pods are on the same subnet, pods on all nodes can communicate with pods on any other node without the aid of Network Address Translation (NAT). Kubernetes also provides a service subnet, which links the stable IP address of a service resource to a set of specified pods. The traffic is forwarded in a transparent way to the pods; an agent (depending on the network mode that you use) manages routing rules to route traffic to the pods that match the service resource selectors. Thus, pods can be treated much like Virtual Machines (VMs) or physical hosts from the perspective of port allocation, networking, naming, service discovery, load balancing, application configuration, and migration. Kubernetes implements this infrastructure by managing the SDN.

The following illustration gives further insight into how the infrastructure components work along with the pod and service subnets to enable network access between pods inside an OpenShift instance.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/sdn-relation.svg)

Figure 4.6: Network access between pods in a cluster

The shared networking namespace of pods enables a straightforward communication model. However, the dynamic nature of pods presents a problem. Pods can be added on the fly to handle increased traffic. Likewise, pods can be dynamically scaled down. If a pod fails, then Kubernetes automatically replaces the pod with a new one. These events change pod IP addresses.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/multicontainer-design-lecture.svg)

Figure 4.7: Problem with direct access to pods

In the diagram, the `Before` side shows the `Front-end` container that is running in a pod with a `10.8.0.1` IP address. The container also refers to a `Back-end` container that is running in a pod with a `10.8.0.2` IP address. In this example, an event occurs that causes the `Back-end` container to fail. A pod can fail for many reasons. In response to the failure, Kubernetes creates a pod for the `Back-end` container that uses a new IP address of `10.8.0.4`. From the `After` side of the diagram, the `Front-end` container now has an invalid reference to the `Back-end` container because of the IP address change. Kubernetes resolves this problem with `service` resources.

#### Using Services

Containers inside Kubernetes pods must not connect directly to each other's dynamic IP address. Instead, Kubernetes assigns a stable IP address to a service resource that is linked to a set of specified pods. The service then acts as a virtual network load balancer for the pods that are linked to the service.

If the pods are restarted, replicated, or rescheduled to different nodes, then the service endpoints are updated, thus providing scalability and fault tolerance for your applications. Unlike the IP addresses of pods, the IP addresses of services do not change.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/multicontainer-fail-with-service.svg)

Figure 4.8: Services resolve pod failure issues

In the diagram, the `Before` side shows that the `Front-end` container now holds a reference to the stable IP address of the `Back-end` service, instead of to the IP address of the pod that is running the `Back-end` container. When the `Back-end` container fails, Kubernetes creates a pod with the `New back-end` container to replace the failed pod. In response to the change, Kubernetes removes the failed pod from the service's host list, or service endpoints, and then adds the IP address of the `New back-end` container pod to the service endpoints. With the addition of the service, requests from the `Front-end` container to the `Back-end` container continue to work, because the service is dynamically updated with the IP address change. A service provides a permanent, static IP address for a group of pods that belong to the same deployment or replica set for an application. Until you delete the service, the assigned IP address does not change, and the cluster does not reuse it.

Most real-world applications do not run as a single pod. Applications need to scale horizontally. Multiple pods run the same containers to meet a growing user demand. A _Deployment_ resource manages multiple pods that execute the same container. A service provides a single IP address for the whole set, and provides load-balancing for client requests among the member pods.

With services, containers in one pod can open network connections to containers in another pod. The pods, which the service tracks, are not required to exist on the same compute node or in the same namespace or project. Because a service provides a stable IP address for other pods to use, a pod also does not need to discover the new IP address of another pod after a restart. The service provides a stable IP address to use, no matter which compute node runs the pod after each restart.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-service-network.svg)

Figure 4.9: Service with pods on many nodes

The _SERVICE_ object provides a stable IP address for the _CLIENT_ container on _NODE X_ to send a request to any one of the _API_ containers.

Kubernetes uses labels on the pods to select the pods that are associated with a service. To include a pod in a service, the pod labels must include each of the `selector` fields of the service.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-service-selector.svg)

Figure 4.10: Service selector match to pod labels

In this example, the selector has a key-value pair of `app: myapp`. Thus, pods with a matching label of `app: myapp` are included in the set that is associated with the service. The _selector_ attribute of a service is used to identify the set of pods that form the endpoints for the service. Each pod in the set is a an endpoint for the service.

To create a service for a deployment, use the `oc expose` command:

[user@host ~]$ **`oc expose deployment/<deployment-name> [--selector <selector>]`**
**`[--port <port>][--target-port <target port>][--protocol <protocol>][--name <name>]`**

The `oc expose` command can use the `--selector` option to specify the label selectors to use. When the command is used without the `--selector` option, the command applies a selector to match the replication controller or replica set.

The `--port` option of the `oc expose` command specifies the port that the service listens on. This port is available only to pods within the cluster. If a port value is not provided, then the port is copied from the deployment configuration.

The `--target-port` option of the `oc expose` command specifies the name or number of the container port that the service uses to communicate with the pods.

The `--protocol` option determines the network protocol for the service. TCP is used by default.

The `--name` option of the `oc expose` command can explicitly name the service. If not specified, the service uses the same name that is provided for the deployment.

To view the selector that a service uses, use the `-o wide` option with the `oc get` command.

[user@host ~]$ **`oc get service db-pod -o wide`**
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP PORT(S)     AGE     SELECTOR
db-pod  ClusterIP   172.30.108.92   <none>      3306/TCP    108s    **`app=db-pod`**

In this example, `db-pod` is the name of the service. Pods must use the `app=db-pod` label to be included in the host list for the `db-pod` service. To see the endpoints that a service uses, use the `oc get endpoints` command.

[user@host ~]$ **`oc get endpoints`**
NAME     ENDPOINTS                       AGE
db-pod   **`10.8.0.86:3306,10.8.0.88:3306`**   27s

This example illustrates a service with two pods in the host list. The `oc get endpoints` command returns the service endpoints in the current selected project. Add the name of the service to the command to show only the endpoints of a single service. Use the `--namepace` option to view the endpoints in a different namespace.

Use the `oc describe deployment <deployment name>` command to view the deployment selector.

[user@host ~]$ **`oc describe deployment db-pod`**
Name:                   db-pod
Namespace:              deploy-services
CreationTimestamp:      Wed, 18 Jan 2023 17:46:03 -0500
Labels:                 app=db-pod
Annotations:            deployment.kubernetes.io/revision: 2
**`Selector:               app=db-pod`**
_...output omitted..._

You can view or parse the selector from the `YAML` or `JSON` output for the deployment resource from the `spec.selector.matchLabels` object. In this example, the `-o yaml` option of the `oc get` command returns the `selector` label that the deployment uses.

[user@host ~]$ **`oc get deployment/<deployment_name> -o yaml`**
..._`output ommitted`_...
  selector:
    matchLabels:
      **`app: db-pod`**
..._`output ommitted`_...

### Kubernetes DNS for Service Discovery

Kubernetes uses an internal Domain Name System (DNS) server that the DNS operator deploys. The DNS operator creates a default cluster DNS name, and assigns DNS names to services that you define. The DNS operator implements the DNS API from the `operator.openshift.io` API group. The operator deploys CoreDNS; creates a service resource for the CoreDNS; and then configures the `kubelet` to instruct pods to use the CoreDNS service IP for name resolution. When a service does not have a cluster IP address, the DNS operator assigns to the service a DNS record that resolves to the set of IP addresses of the pods behind the service.

The DNS server discovers a service from a pod by using the internal DNS server, which is visible only to pods. Each service is dynamically assigned a _Fully Qualified Domain Name_ (FQDN) that uses the following format:

_`SVC-NAME`_._`PROJECT-NAME`_.svc._`CLUSTER-DOMAIN`_

When a pod is created, Kubernetes provides the container with a `/etc/resolv.conf` file with similar contents to the following items:

[user@host ~]$ **`cat /etc/resolv.conf`**
search deploy-services.svc.cluster.local svc.cluster.local ...
nameserver 172.30.0.10
options ndots:5

In this example, `deploy-services` is the project name for the pod, and `cluster.local` is the cluster domain.

The `nameserver` directive provides the IP address of the Kubernetes internal DNS server. The `options ndots` directive specifies the number of dots that must appear in a name to qualify for an initial absolute query. Alternative hostname values are derived by appending values from the `Search` directive to the name that is sent to the DNS server.

In the `search` directive in this example, the `svc.cluster.local` entry enables any pod to communicate with another pod in the same cluster by using the service name and project name:

_`SVC-NAME`_._`PROJECT-NAME`_

The first entry in the `search` directive enables a pod to use the service name to specify another pod in the same project. In RHOCP, a project is also the namespace for the pod. The service name alone is sufficient for pods in the same RHOCP project:

_`SVC-NAME`_

### Kubernetes Networking Drivers

Container Network Interface (CNI) plug-ins provide a common interface between the network provider and the container runtime. CNI defines the specifications for plug-ins that configure network interfaces inside containers. Plug-ins that are written to the specification enable different network providers to control the RHOCP cluster network.

Red Hat provides the following CNI plug-ins for a RHOCP cluster:

- OVN-Kubernetes: The default plug-in for first-time installations of RHOCP, starting with RHOCP 4.10.
    
- OpenShift SDN: An earlier plug-in from RHOCP 3.x; it is incompatible with some later features of RHOCP 4.x.
    
- Kuryr: A plug-in for integration and performance on OpenStack deployments.
    

Certified CNI-plugins from other vendors are also compatible with an RHOCP cluster.

The SDN uses CNI plug-ins to create Linux namespaces to partition the usage of resources and processes on physical and virtual hosts. With this implementation, containers inside pods can share network resources, such as devices, IP stacks, firewall rules, and routing tables. The SDN allocates a unique routable IP to each pod, so that you can access the pod from any other service in the same network.

In OpenShift 4.14, OVN-Kubernetes is the default network provider.

OVN-Kubernetes uses Open Virtual Network (OVN) to manage the cluster network. A cluster that uses the OVN-Kubernetes plug-in also runs Open vSwitch (OVS) on each node. OVN configures OVS on each node to implement the declared network configuration.

#### The OpenShift Cluster Network Operator

RHOCP provides a _Cluster Network Operator_ (CNO) that configures OpenShift cluster networking. The CNO is a OpenShift cluster operator that loads and configures Container Network Interface (CNI) plug-ins. As a cluster administrator, execute the following command to observe the status of the CNO:

```bash
[user@host ~]$ **`oc get -n openshift-network-operator deployment/network-operator`**
NAME              READY   UP-TO-DATE  AVAILABLE   AGE
network-operator  1/1     1           1           41d
```

An administrator configures the cluster network operator at installation time. To see the configuration, use the following command:

```bash
[user@host ~]$ **`oc describe network.config/cluster`**
Name: cluster
_...output omitted..._
Spec:
Cluster Network:
Cidr: 10.8.0.0/14 ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
Host Prefix: 23
External IP:
Policy:
Network Type: OVNKubernetes
Service Network:
172.30.0.0/16 ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
_...output omitted..._
```

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_the_openshift_cluster_network_operator-CO15-1)|The Cluster Network CIDR defines the range of IPs for all pods in the cluster.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_the_openshift_cluster_network_operator-CO15-2)|The Service Network CIDR defines the range of IPs for all services in the cluster.|

## Kubernetes Pod and Service Networks

### Objectives

- Interconnect applications pods inside the same cluster by using Kubernetes services.
    

### The Software-defined Network

Kubernetes implements software-defined networking (SDN) to manage the network infrastructure of the cluster and user applications. The SDN is a virtual network that encompasses all cluster nodes. The virtual network enables communication between any container or pod inside the cluster. Cluster node processes that Kubernetes pods manage can access the SDN. However, the SDN is not accessible from outside the cluster, nor to regular processes on cluster nodes. With the software-defined networking model, you can manage network services through the abstraction of several networking layers.

With the SDN, you can manage the network traffic and network resources programmatically, so that the organization teams can decide how to expose their applications. The SDN implementation creates a model that is compatible with traditional networking practices. It makes pods akin to virtual machines in terms of port allocation, IP address leasing, and reservation.

With the SDN design, you do not need to change how application components communicate with each other, which helps to containerize legacy applications. If your application is composed of many services that communicate over the TCP/UDP stack, then this approach still works, because containers in a pod use the same network stack.

The following diagram shows how all pods are connected to a shared network:

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-pod-network.svg)|

Figure 4.5: How the Kubernetes SDN manages the network

Among the many features of SDN, with open standards, vendors can propose their solutions for centralized management, dynamic routing, and tenant isolation.

### Kubernetes Networking

Networking in Kubernetes provides a scalable means of communication between containers.

Kubernetes networking provides the following capabilities:

- Highly coupled container-to-container communications
    
- Pod-to-pod communications
    
- Pod-to-service communications
    
- External-to-service communication: covered in [the section called “ Scale and Expose Applications to External Access ”](https://rol.redhat.com/rol/app/courses/do180-4.14/pages/ch04s07)
    

Kubernetes automatically assigns an IP address to every pod. However, pod IP addresses are unstable, because pods are ephemeral. Pods are constantly created and destroyed across the nodes in the cluster. For example, when you deploy a new version of your application, Kubernetes destroys the existing pods and then deploys new ones.

All containers within a pod share networking resources. The IP address and MAC address that are assigned to the pod are shared among all containers in the pod. Thus, all containers within a pod can reach each other's ports through the loopback address, `localhost`. Ports that are bound to localhost are available to all containers that run within the pod, but never to containers outside it.

By default, the pods can communicate with each other even if they run on different cluster nodes or belong to different Kubernetes namespaces. Every pod is assigned an IP address in a flat shared networking namespace that has full communication with other physical computers and containers across the network. All pods are assigned a unique IP address from a Classless Inter-Domain Routing (CIDR) range of host addresses. The shared address range places all pods in the same subnet.

Because all the pods are on the same subnet, pods on all nodes can communicate with pods on any other node without the aid of Network Address Translation (NAT). Kubernetes also provides a service subnet, which links the stable IP address of a service resource to a set of specified pods. The traffic is forwarded in a transparent way to the pods; an agent (depending on the network mode that you use) manages routing rules to route traffic to the pods that match the service resource selectors. Thus, pods can be treated much like Virtual Machines (VMs) or physical hosts from the perspective of port allocation, networking, naming, service discovery, load balancing, application configuration, and migration. Kubernetes implements this infrastructure by managing the SDN.

The following illustration gives further insight into how the infrastructure components work along with the pod and service subnets to enable network access between pods inside an OpenShift instance.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/sdn-relation.svg)

Figure 4.6: Network access between pods in a cluster

The shared networking namespace of pods enables a straightforward communication model. However, the dynamic nature of pods presents a problem. Pods can be added on the fly to handle increased traffic. Likewise, pods can be dynamically scaled down. If a pod fails, then Kubernetes automatically replaces the pod with a new one. These events change pod IP addresses.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/multicontainer-design-lecture.svg)

Figure 4.7: Problem with direct access to pods

In the diagram, the `Before` side shows the `Front-end` container that is running in a pod with a `10.8.0.1` IP address. The container also refers to a `Back-end` container that is running in a pod with a `10.8.0.2` IP address. In this example, an event occurs that causes the `Back-end` container to fail. A pod can fail for many reasons. In response to the failure, Kubernetes creates a pod for the `Back-end` container that uses a new IP address of `10.8.0.4`. From the `After` side of the diagram, the `Front-end` container now has an invalid reference to the `Back-end` container because of the IP address change. Kubernetes resolves this problem with `service` resources.

#### Using Services

Containers inside Kubernetes pods must not connect directly to each other's dynamic IP address. Instead, Kubernetes assigns a stable IP address to a service resource that is linked to a set of specified pods. The service then acts as a virtual network load balancer for the pods that are linked to the service.

If the pods are restarted, replicated, or rescheduled to different nodes, then the service endpoints are updated, thus providing scalability and fault tolerance for your applications. Unlike the IP addresses of pods, the IP addresses of services do not change.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/multicontainer-fail-with-service.svg)

Figure 4.8: Services resolve pod failure issues

In the diagram, the `Before` side shows that the `Front-end` container now holds a reference to the stable IP address of the `Back-end` service, instead of to the IP address of the pod that is running the `Back-end` container. When the `Back-end` container fails, Kubernetes creates a pod with the `New back-end` container to replace the failed pod. In response to the change, Kubernetes removes the failed pod from the service's host list, or service endpoints, and then adds the IP address of the `New back-end` container pod to the service endpoints. With the addition of the service, requests from the `Front-end` container to the `Back-end` container continue to work, because the service is dynamically updated with the IP address change. A service provides a permanent, static IP address for a group of pods that belong to the same deployment or replica set for an application. Until you delete the service, the assigned IP address does not change, and the cluster does not reuse it.

Most real-world applications do not run as a single pod. Applications need to scale horizontally. Multiple pods run the same containers to meet a growing user demand. A _Deployment_ resource manages multiple pods that execute the same container. A service provides a single IP address for the whole set, and provides load-balancing for client requests among the member pods.

With services, containers in one pod can open network connections to containers in another pod. The pods, which the service tracks, are not required to exist on the same compute node or in the same namespace or project. Because a service provides a stable IP address for other pods to use, a pod also does not need to discover the new IP address of another pod after a restart. The service provides a stable IP address to use, no matter which compute node runs the pod after each restart.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-service-network.svg)

Figure 4.9: Service with pods on many nodes

The _SERVICE_ object provides a stable IP address for the _CLIENT_ container on _NODE X_ to send a request to any one of the _API_ containers.

Kubernetes uses labels on the pods to select the pods that are associated with a service. To include a pod in a service, the pod labels must include each of the `selector` fields of the service.

![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/services/assets/network-sdn-service-selector.svg)

Figure 4.10: Service selector match to pod labels

In this example, the selector has a key-value pair of `app: myapp`. Thus, pods with a matching label of `app: myapp` are included in the set that is associated with the service. The _selector_ attribute of a service is used to identify the set of pods that form the endpoints for the service. Each pod in the set is a an endpoint for the service.

To create a service for a deployment, use the `oc expose` command:

[user@host ~]$ **`oc expose deployment/<deployment-name> [--selector <selector>]`**
**`[--port <port>][--target-port <target port>][--protocol <protocol>][--name <name>]`**

The `oc expose` command can use the `--selector` option to specify the label selectors to use. When the command is used without the `--selector` option, the command applies a selector to match the replication controller or replica set.

The `--port` option of the `oc expose` command specifies the port that the service listens on. This port is available only to pods within the cluster. If a port value is not provided, then the port is copied from the deployment configuration.

The `--target-port` option of the `oc expose` command specifies the name or number of the container port that the service uses to communicate with the pods.

The `--protocol` option determines the network protocol for the service. TCP is used by default.

The `--name` option of the `oc expose` command can explicitly name the service. If not specified, the service uses the same name that is provided for the deployment.

To view the selector that a service uses, use the `-o wide` option with the `oc get` command.

[user@host ~]$ **`oc get service db-pod -o wide`**
NAME    TYPE        CLUSTER-IP      EXTERNAL-IP PORT(S)     AGE     SELECTOR
db-pod  ClusterIP   172.30.108.92   <none>      3306/TCP    108s    **`app=db-pod`**

In this example, `db-pod` is the name of the service. Pods must use the `app=db-pod` label to be included in the host list for the `db-pod` service. To see the endpoints that a service uses, use the `oc get endpoints` command.

[user@host ~]$ **`oc get endpoints`**
NAME     ENDPOINTS                       AGE
db-pod   **`10.8.0.86:3306,10.8.0.88:3306`**   27s

This example illustrates a service with two pods in the host list. The `oc get endpoints` command returns the service endpoints in the current selected project. Add the name of the service to the command to show only the endpoints of a single service. Use the `--namepace` option to view the endpoints in a different namespace.

Use the `oc describe deployment <deployment name>` command to view the deployment selector.

[user@host ~]$ **`oc describe deployment db-pod`**
Name:                   db-pod
Namespace:              deploy-services
CreationTimestamp:      Wed, 18 Jan 2023 17:46:03 -0500
Labels:                 app=db-pod
Annotations:            deployment.kubernetes.io/revision: 2
**`Selector:               app=db-pod`**
_...output omitted..._

You can view or parse the selector from the `YAML` or `JSON` output for the deployment resource from the `spec.selector.matchLabels` object. In this example, the `-o yaml` option of the `oc get` command returns the `selector` label that the deployment uses.

[user@host ~]$ **`oc get deployment/<deployment_name> -o yaml`**
..._`output ommitted`_...
  selector:
    matchLabels:
      **`app: db-pod`**
..._`output ommitted`_...

### Kubernetes DNS for Service Discovery

Kubernetes uses an internal Domain Name System (DNS) server that the DNS operator deploys. The DNS operator creates a default cluster DNS name, and assigns DNS names to services that you define. The DNS operator implements the DNS API from the `operator.openshift.io` API group. The operator deploys CoreDNS; creates a service resource for the CoreDNS; and then configures the `kubelet` to instruct pods to use the CoreDNS service IP for name resolution. When a service does not have a cluster IP address, the DNS operator assigns to the service a DNS record that resolves to the set of IP addresses of the pods behind the service.

The DNS server discovers a service from a pod by using the internal DNS server, which is visible only to pods. Each service is dynamically assigned a _Fully Qualified Domain Name_ (FQDN) that uses the following format:

_`SVC-NAME`_._`PROJECT-NAME`_.svc._`CLUSTER-DOMAIN`_

When a pod is created, Kubernetes provides the container with a `/etc/resolv.conf` file with similar contents to the following items:

[user@host ~]$ **`cat /etc/resolv.conf`**
search deploy-services.svc.cluster.local svc.cluster.local ...
nameserver 172.30.0.10
options ndots:5

In this example, `deploy-services` is the project name for the pod, and `cluster.local` is the cluster domain.

The `nameserver` directive provides the IP address of the Kubernetes internal DNS server. The `options ndots` directive specifies the number of dots that must appear in a name to qualify for an initial absolute query. Alternative hostname values are derived by appending values from the `Search` directive to the name that is sent to the DNS server.

In the `search` directive in this example, the `svc.cluster.local` entry enables any pod to communicate with another pod in the same cluster by using the service name and project name:

_`SVC-NAME`_._`PROJECT-NAME`_

The first entry in the `search` directive enables a pod to use the service name to specify another pod in the same project. In RHOCP, a project is also the namespace for the pod. The service name alone is sufficient for pods in the same RHOCP project:

_`SVC-NAME`_

### Kubernetes Networking Drivers

Container Network Interface (CNI) plug-ins provide a common interface between the network provider and the container runtime. CNI defines the specifications for plug-ins that configure network interfaces inside containers. Plug-ins that are written to the specification enable different network providers to control the RHOCP cluster network.

Red Hat provides the following CNI plug-ins for a RHOCP cluster:

- OVN-Kubernetes: The default plug-in for first-time installations of RHOCP, starting with RHOCP 4.10.
    
- OpenShift SDN: An earlier plug-in from RHOCP 3.x; it is incompatible with some later features of RHOCP 4.x.
    
- Kuryr: A plug-in for integration and performance on OpenStack deployments.
    

Certified CNI-plugins from other vendors are also compatible with an RHOCP cluster.

The SDN uses CNI plug-ins to create Linux namespaces to partition the usage of resources and processes on physical and virtual hosts. With this implementation, containers inside pods can share network resources, such as devices, IP stacks, firewall rules, and routing tables. The SDN allocates a unique routable IP to each pod, so that you can access the pod from any other service in the same network.

In OpenShift 4.14, OVN-Kubernetes is the default network provider.

OVN-Kubernetes uses Open Virtual Network (OVN) to manage the cluster network. A cluster that uses the OVN-Kubernetes plug-in also runs Open vSwitch (OVS) on each node. OVN configures OVS on each node to implement the declared network configuration.

#### The OpenShift Cluster Network Operator

RHOCP provides a _Cluster Network Operator_ (CNO) that configures OpenShift cluster networking. The CNO is a OpenShift cluster operator that loads and configures Container Network Interface (CNI) plug-ins. As a cluster administrator, execute the following command to observe the status of the CNO:

[user@host ~]$ **`oc get -n openshift-network-operator deployment/network-operator`**
NAME              READY   UP-TO-DATE  AVAILABLE   AGE
network-operator  1/1     1           1           41d

An administrator configures the cluster network operator at installation time. To see the configuration, use the following command:

[user@host ~]$ **`oc describe network.config/cluster`**
Name: cluster
_...output omitted..._
Spec:
Cluster Network:
Cidr: 10.8.0.0/14 ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
Host Prefix: 23
External IP:
Policy:
Network Type: OVNKubernetes
Service Network:
172.30.0.0/16 ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
_...output omitted..._

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_the_openshift_cluster_network_operator-CO15-1)|The Cluster Network CIDR defines the range of IPs for all pods in the cluster.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_the_openshift_cluster_network_operator-CO15-2)|The Service Network CIDR defines the range of IPs for all services in the cluster.|


## Scale and Expose Applications to External Access

### Objectives

- Expose applications to clients outside the cluster by using Kubernetes ingress and OpenShift routes.
    

### IP Addresses for Pods and Services

Most real-world applications do not run as a single pod. Because applications need to scale horizontally, many pods run the same containers from the same pod resource definition, to meet growing user demand. A service defines a single IP/port combination, and provides a single IP address to a pool of pods, and a load-balancing client request among member pods.

By default, services connect clients to pods in a round-robin fashion, and each service is assigned a unique IP address for clients to connect to. This IP address comes from an internal OpenShift virtual network, which although distinct from the pods' internal network, is visible only to pods. Each pod that matches the selector is added to the service resource as an endpoint.

Containers inside Kubernetes pods must not connect to each other's dynamic IP address directly. Services resolve this problem by linking more stable IP addresses from the SDN to the pods. If pods are restarted, replicated, or rescheduled to different nodes, then services are updated, to provide scalability and fault tolerance.

### Service Types

You can choose between several service types depending on your application needs, cluster infrastructure, and security requirements.

ClusterIP

This type is the default, unless you explicitly specify a type for a service. The ClusterIP type exposes the service on a cluster-internal IP address. If you choose this value, then the service is reachable only from within the cluster.

The ClusterIP service type is used for pod-to-pod routing within the RHOCP cluster, and enables pods to communicate with and to access each other. IP addresses for the ClusterIP services are assigned from a dedicated service network that is accessible only from inside the cluster. Most applications should use this service type, for which Kubernetes automates the management.

Load balancer

This resource instructs RHOCP to activate a load balancer in a cloud environment. A load balancer instructs Kubernetes to interact with the cloud provider that the cluster is running in, to provision a load balancer. The load balancer then provides an externally accessible IP address to the application.

Take all necessary precautions before deploying this service type. Load balancers are typically too expensive to assign one for each application in a cluster. Furthermore, applications that use this service type become accessible from networks outside the cluster. Additional security configuration is required to prevent unintended access.

NodePort

With this method, Kubernetes exposes a service on a port on the node IP address. The port is exposed on all cluster nodes, and each node redirects traffic to the endpoints (pods) of the service.

A NodePort service requires allowing direct network connections to a cluster node, which is a security risk.

ExternalName

This service tells Kubernetes that the DNS name in the `externalName` field is the location of the resource that backs the service. When a DNS request is made against the Kubernetes DNS server, it returns the `externalName` in a _Canonical Name (CNAME)_ record, and directs the client to look up the returned name to get the IP address.

### Using Routes for External Connectivity

RHOCP provides resources to expose your applications to external networks outside the cluster. You can expose HTTP and HTTPS traffic, TCP applications, and also non-TCP traffic. However, you should expose only HTTP and TLS-based applications to external access. Applications that use other protocols, such as databases, are usually not exposed to external access (from outside a cluster). Routes and ingress are the main resources for handling ingress traffic.

RHOCP provides the `route` resource to expose your applications to external networks. With routes, you can access your application with a unique hostname that is publicly accessible. Routes rely on a Kubernetes ingress controller to redirect the traffic from the public IP address to pods. By default, Kubernetes provides an ingress controller, starting from the 1.24 release. For RHOCP clusters, the OpenShift ingress operator provides the ingress controller. RHOCP clusters can also use various third-party ingress controllers that can be deployed in parallel with the OpenShift ingress controller.

Routes provide ingress traffic to services in the cluster. Routes were created before Kubernetes ingress objects, and provide more features. Routes provide advanced features that Kubernetes ingress controllers might not support through a standard interface, such as TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.

To create a route with the `oc` CLI, use the ``oc expose service _`service-name`_`` command. Include the `--hostname` option to provide a custom hostname for the route.

[user@host ~]$ **`oc expose service api-frontend \ --hostname api.apps.acme.com`**

If you omit the hostname, then RHOCP automatically generates a hostname with the following structure: `<route-name>-<project-name>.<default-domain>`. For example, if you create a `frontend` route in an `api` project, in a cluster that uses `apps.example.com` as the wildcard domain, then the route hostname is as follows:

```
frontend-api.apps.example.com
```

### Important

The DNS server that hosts the wildcard domain is unaware of any route hostnames; it resolves any name only to the configured IPs. Only the RHOCP router knows about route hostnames, and treats each one as an HTTP virtual host.

Invalid wildcard domain hostnames, or hostnames that do not correspond to any route, are blocked by the RHOCP router and result in an HTTP 503 error.

Consider the following settings when creating a route:

- The name of a service. The route uses the service to determine the pods to direct the traffic to.
    
- A hostname for the route. A route is always a subdomain of your cluster wildcard domain. For example, if you are using a wildcard domain of `apps.dev-cluster.acme.com`, and need to expose a `frontend` service through a route, then the route name is as follows:
    
    ```
    frontend.apps.dev-cluster.acme.com.
    ```
    
    RHOCP can also automatically generate a hostname for the route.
    
- An optional path, for path-based routes.
    
- A target port that the application listens to. The target port corresponds to the port that you define in the `targetPort` key of the service.
    
- An encryption strategy, depending on whether you need a secure or an insecure route.
    

The following listing shows a minimal definition for a route:

kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: a-simple-route ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
  labels: ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
    app: API
    name: api-frontend
spec:
  host: api.apps.acme.com ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
  to:
    kind: Service
    name: api-frontend ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
  port: 8080 ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
    targetPort: 8443

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_using_routes_for_external_connectivity-CO16-1)|The name of the route. This name must be unique.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_using_routes_for_external_connectivity-CO16-2)|A set of labels that you can use as selectors.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_using_routes_for_external_connectivity-CO16-3)|The hostname of the route. This hostname must be a subdomain of your wildcard domain, because RHOCP routes the wildcard domain to the routers.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_using_routes_for_external_connectivity-CO16-4)|The service to redirect the traffic to. Although you use a service name, the route uses this information only to determine the list of pods that receive the traffic.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_using_routes_for_external_connectivity-CO16-5)|Port mapping from a router to an endpoint in the service endpoints. The target port on pods that are selected by the service that this route points to.|

**Note

Some ecosystem components have an integration with ingress resources, but not with route resources. For this case, RHOCP automatically creates managed route objects when an ingress object is created. These route objects are deleted when the corresponding ingress objects are deleted.

You can delete a route by using the ``oc delete route _`route-name`_`` command.

[user@host ~]$ **`oc delete route myapp-route`**

You can also expose a service from the web console by clicking the Networking → Routes menu. Click Create Route and customize the name, the hostname, the path, and the service to route to by using the form view or the YAML manifest.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/deploy/routes/assets/routeform.png)|

### Using Ingress Objects for External Connectivity

An ingress is a Kubernetes resource that provides some of the same features as routes (which are an RHOCP resource). Ingress objects accept external requests and transfer the requests based on the route. You can enable only certain types of traffic: HTTP, HTTPS and server name identification (SNI), and TLS with SNI. Standard Kubernetes ingress resources are typically minimal. Many common features that applications rely on, such as TLS termination, path redirecting, and sticky sessions, depend on the ingress controller. Kubernetes does not define the configuration syntax. In RHOCP, routes are generated to meet the conditions that the ingress object specifies.

**Note

The ingress resource is commonly used for Kubernetes. However, the route resource is the preferred method for external connectivity in RHOCP.

To create an ingress object, use the `oc create ingress ingress-name --rule=URL_route=service-name:port-number` command. Use the `--rule` option to provide a custom rule in the `host/path=service:port[,tls=secretname]` format. If the TLS option is omitted, then an insecure route is created.

[user@host ~]$ **`oc create ingress ingr-sakila \ --rule="ingr-sakila.apps.ocp4.example.com/*=sakila-service:8080"`**

The following listing shows a minimal definition for an ingress object:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
spec:
  rules: ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
  - host: "www.example.com ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
    http:
      paths:
      - backend: ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
          service:
            name: frontend
            port:
              number: 80
        pathType: Prefix
        path: /
  tls: ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
  - hosts:
    - www.example.com
    secretName: example-com-tls-certificate

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_using_ingress_objects_for_external_connectivity-CO17-1)|The name of the ingress object. This name must be unique.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_using_ingress_objects_for_external_connectivity-CO17-2)|The HTTP or HTTPS rule for the ingress object.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_using_ingress_objects_for_external_connectivity-CO17-3)|The host for the ingress object. Applies the HTTP rule to the inbound HTTP traffic of the specified host.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_using_ingress_objects_for_external_connectivity-CO17-4)|The backend to redirect traffic to. Defines the service name, port number, and port names for the ingress object. To connect to the back end, incoming requests must match the host and path of the rule.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_using_ingress_objects_for_external_connectivity-CO17-5)|The configuration of TLS for the ingress object; it is required for secured paths. The host in the TLS object must match the host in the rules object.|

You can delete an ingress object by using the ``oc delete ingress _`ingress-name`_`` command.

[user@host ~]$ **`oc delete ingress example-ingress`**

### Sticky Sessions

Sticky sessions enable stateful application traffic by ensuring that all requests reach the same endpoint. RHOCP uses cookies to configure session persistence for ingress and route resources. The ingress controller selects an endpoint to handle any user requests, and creates a cookie for the session. The cookie is passed back in response to the request, and the user sends back the cookie with the next request in the session. The cookie tells the ingress controller which endpoint is handling the session, to ensure that client requests use the cookie so that they are routed to the same pod.

RHOCP auto-generates the cookie name for ingress and route resources. You can overwrite the default cookie name by using the `annotate` command with either the `kubectl` or the `oc` commands. With this annotation, the application that receives route traffic knows the cookie name.

The following example configures a cookie for an ingress object:

[user@host ~]$ **`oc annotate ingress ingr-example \ ingress.kubernetes.io/affinity=cookie`**

The following example configures a cookie named `myapp` for a route object:

[user@host ~]$ **`oc annotate route route-example \ router.openshift.io/cookie_name=myapp`**

After you annotate the route, capture the route hostname in a variable:

[user@host ~]$ **`ROUTE_NAME=$(oc get route <route_name> \ -o jsonpath='{.spec.host}')`**

Then, use the `curl` command to save the cookie and access the route:

[user@host ~]$ **`curl $ROUTE_NAME -k -c /tmp/cookie_jar`**

The cookie is passed back in response to the request, and is saved to the `/tmp/cookie_jar` directory. Use the `curl` command and the cookie that was saved from the previous command to connect to the route:

[user@host ~]$ **`curl $ROUTE_NAME -k -b /tmp/cookie_jar`**

By using the saved cookie, the request is sent to the same pod as the previous request.

### Load Balance and Scale Applications

Developers and administrators can choose to manually scale the number of replica pods in a deployment. More pods might be needed for an anticipated surge in traffic, or the pod count might be reduced to reclaim resources that the cluster can use elsewhere.

You can change the number of replicas in a deployment resource manually by using the `oc scale` command.

[user@host ~]$ **`oc scale --replicas 5 deployment/scale`**

The deployment resource propagates the change to the replica set. The replica set reacts to the change by creating pods (replicas) or by deleting existing ones, depending on whether the new intended replica count is less than or greater than the existing count.

Although you can manipulate a replica set resource directly, the recommended practice is to manipulate the deployment resource instead. A new deployment creates either a replica set or a replication controller, and direct changes to a previous replica set or replication controller are ignored.

#### Load Balance Pods

A Kubernetes service serves as an internal load balancer. Standard services act as a load balancer or a proxy, and give access to the workload object by using the service name. A service identifies a set of replicated pods to transfer the connections that it receives.

A router uses the service selector to find the service and the endpoints, or pods, that back the service. When both a router and a service provide load balancing, RHOCP uses the router to load-balance traffic to pods. A router detects relevant changes in the IP addresses of its services, and adapts its configuration accordingly. Custom routers can thereby communicate modifications of API objects to an external routing solution.

RHOCP routers map external hostnames, and load-balance service endpoints over protocols that pass distinguishing information directly to the router. The hostname must exist in the protocol for the router to determine where to send it.