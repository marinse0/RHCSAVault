## Application High Availability with Kubernetes

### Objectives

- Describe how Kubernetes tries to keep applications running after failures.
    

### Concepts of Deploying Highly Available Applications

_High availability_ (HA) is a goal of making applications more robust and resistant to runtime failures. Implementing HA techniques decreases the likelihood that an application is completely unavailable to users.

In general, HA can protect an application from failures in the following contexts:

- From itself in the form of application bugs
    
- From its environment, such as networking issues
    
- From other applications that exhaust cluster resources
    

Additionally, HA practices can protect the cluster from applications, such as one with a memory leak.

### Writing Reliable Applications

At its core, cluster-level HA tooling mitigates worst-case scenarios. HA is not a substitute for fixing application-level issues, but augments developer mitigations. Although required for reliability, application security is a separate concern.

Applications must work with the cluster so that Kubernetes can best handle failure scenarios. Kubernetes expects the following behaviors from applications:

- Tolerates restarts
    
- Responds to health probes, such as the startup, readiness, and liveness probes
    
- Supports multiple simultaneous instances
    
- Has well-defined and well-behaved resource usage
    
- Operates with restricted privileges
    

Although the cluster can run applications that lack the preceding behaviors, applications with these behaviors better use the reliability and HA features that Kubernetes provides.

Most HTTP-based applications provide an endpoint to verify application health. The cluster can be configured to observe this endpoint and mitigate potential issues for the application.

The application is responsible for providing such an endpoint. Developers must decide how the application determines its state.

For example, if an application depends on a database connection, then the application might respond with a healthy status only when the database is reachable. However, not all applications that make database connections need such a check. This decision is at the discretion of the developers.

### Kubernetes Application Reliability

If an application pod crashes, then it cannot respond to requests. Depending on the configuration, the cluster can automatically restart the pod. If the application fails without crashing the pod, then the pod does not receive requests. However, the cluster can do so only with the appropriate health probes.

Kubernetes uses the following HA techniques to improve application reliability:

- Restarting pods: By configuring a restart policy on a pod, the cluster restarts misbehaving instances of an application.
    
- Probing: By using health probes, the cluster knows when applications cannot respond to requests, and can automatically act to mitigate the issue.
    
- Horizontal scaling: When the application load changes, the cluster can scale the number of replicas to match the load.
    

These techniques are explored throughout this chapter.


## Application Health Probes

### Objectives

- Describe how Kubernetes uses health probes during deployment, scaling, and failover of applications.
    

### Kubernetes Probes

Health probes are an important part of maintaining a robust cluster. _Probes_ enable the cluster to determine the status of an application by repeatedly probing it for a response.

A set of health probes affect a cluster's ability to do the following tasks:

- Crash mitigation by automatically attempting to restart failing pods
    
- Failover and load balancing by sending requests only to healthy pods
    
- Monitoring by determining whether and when pods are failing
    
- Scaling by determining when a new replica is ready to receive requests
    

### Authoring Probe Endpoints

Application developers are expected to code health probe endpoints during application development. These endpoints determine the health and status of the application. For example, a data-driven application might report a successful health probe only if it can connect to the database.

Because the cluster calls them often, health probe endpoints should be quick to perform. Endpoints should not perform complicated database queries or many network calls.

### Probe Types

Kubernetes provides the following types of probes: startup, readiness, and liveness. Depending on the application, you might configure one or more of these types.

#### Readiness Probes

A _readiness probe_ determines whether the application is ready to serve requests. If the readiness probe fails, then Kubernetes prevents client traffic from reaching the application by removing the pod's IP address from the service resource.

Readiness probes help to detect temporary issues that might affect your applications. For example, the application might be temporarily unavailable when it starts, because it must establish initial network connections, load files in a cache, or perform initial tasks that take time to complete. The application might occasionally need to run long batch jobs, which make it temporarily unavailable to clients.

Kubernetes continues to run the probe even after the application fails. If the probe succeeds again, then Kubernetes adds back the pod's IP address to the service resource, and requests are sent to the pod again.

In such cases, the readiness probe addresses a temporary issue and improves application availability.

#### Liveness Probes

Like a readiness probe, a _liveness probe_ is called throughout the lifetime of the application. Liveness probes determine whether the application container is in a healthy state. If an application fails its liveness probe enough times, then the cluster restarts the pod according to its restart policy.

Unlike a startup probe, liveness probes are called after the application's initial start process. Usually, this mitigation is by restarting or re-creating the pod.

#### Startup Probes

A _startup probe_ determines when an application's startup is completed. Unlike a liveness probe, a startup probe is not called after the probe succeeds. If the startup probe does not succeed after a configurable timeout, then the pod is restarted based on its `restartPolicy` value.

Consider adding a startup probe to applications with a long start time. By using a startup probe, the liveness probe can remain short and responsive.

### Types of Tests

When defining a probe, you must specify one of the following types of test to perform:

HTTP GET

Each time that the probe runs, the cluster sends a request to the specified HTTP endpoint. The test is considered a success if the request responds with an HTTP response code between `200` and `399`. Other responses cause the test to fail.

Container command

Each time that the probe runs, the cluster runs the specified command in the container. If the command exits with a status code of `0`, then the test succeeds. Other status codes cause the test to fail.

TCP socket

Each time that the probe runs, the cluster attempts to open a socket to the container. The test succeeds only if the connection is established.

### Timings and Thresholds

All the types of probes include timing variables. The _period seconds_ variable defines how often the probe runs. The _failure threshold_ defines how many failed attempts are required before the probe itself fails.

For example, a probe with a failure threshold of `3` and period seconds of `5` can fail up to three times before the overall probe fails. Using this probe configuration means that the issue can exist for 10 seconds before it is mitigated. However, running probes too often can waste resources. Consider these values when setting probes.

### Adding Probes via YAML

Because probes are defined on a pod template, probes can be added to workload resources such as deployments. To add a probe to an existing deployment, update and apply the YAML file or use the `oc edit` command. For example, the following YAML excerpt defines a deployment pod template with a probe:

apiVersion: apps/v1
kind: Deployment
_...output omitted..._
spec:
_...output omitted..._
  template:
    spec:
      containers:
      - name: web-server
        _...output omitted..._
        livenessProbe: ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)
          failureThreshold: 6 ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)
          periodSeconds: 10 ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)
          httpGet: ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)
            path: /health ![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)
            port: 3000 ![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-1)|Defines a liveness probe.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-2)|Specifies how many times the probe must fail before mitigating.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-3)|Defines how often the probe runs.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-4)|Sets the probe as an HTTP request and defines the request port and path.|
|[![5](https://rol.redhat.com/rol/static/roc/Common_Content/images/5.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-5)|Specifies the HTTP path to send the request to.|
|[![6](https://rol.redhat.com/rol/static/roc/Common_Content/images/6.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_yaml-CO32-6)|Specifies the port to send the HTTP request over.|

### Adding Probes via the CLI

The `oc set probe` command adds or modifies a probe on a deployment. For example, the following command adds a readiness probe to a deployment called `front-end`:

[user@host ~]$ **`oc set probe deployment/front-end \ --readiness \ ![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg) --failure-threshold 6 \ ![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg) --period-seconds 10 \ ![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg) --get-url http://:8080/healthz`** ![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)

|   |   |
|---|---|
|[![1](https://rol.redhat.com/rol/static/roc/Common_Content/images/1.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_the_cli-CO33-1)|Defines a readiness probe.|
|[![2](https://rol.redhat.com/rol/static/roc/Common_Content/images/2.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_the_cli-CO33-2)|Sets how many times the probe must fail before mitigating.|
|[![3](https://rol.redhat.com/rol/static/roc/Common_Content/images/3.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_the_cli-CO33-3)|Sets how often the probe runs.|
|[![4](https://rol.redhat.com/rol/static/roc/Common_Content/images/4.svg)](https://rol.redhat.com/rol/app/#_adding_probes_via_the_cli-CO33-4)|Sets the probe as an HTTP request, and defines the request port and path.|

### Adding Probes via the Web Console

To add or modify a probe on a deployment from the web console, navigate to the Workloads → Deployments menu and select a deployment.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/reliability/probes/assets/deployments.png)|

Click Actions and then click Add Health Checks.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/reliability/probes/assets/addprobes.png)|

Click Edit probe to specify the readiness type, the HTTP headers, the path, the port, and more.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/reliability/probes/assets/editprobes.png)|

---
**Note

The `set probe` command is exclusive to RHOCP and `oc`.

---
