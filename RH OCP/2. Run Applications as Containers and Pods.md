## Create Linux Containers and Kubernetes Pods

### Objectives

- Run containers inside pods and identify the host OS processes and namespaces that the containers use.
    

### Creating Containers and Pods

Kubernetes and OpenShift offer many ways to create containers in pods. You can use one such way, the `run` command, with the `kubectl` or `oc` CLI to create and deploy an application in a pod from a container image. A _container image_ contains immutable data that defines an application and its libraries.

**Note

Container images are discussed in more detail elsewhere in the course.

The `run` command uses the following syntax:

**``oc run _`RESOURCE/NAME`_ --image _`IMAGE`_ [options]``**

For example, the following command deploys an Apache HTTPD application in a pod named `web-server` that uses the `registry.access.redhat.com/ubi8/httpd-24` container image.

[user@host ~]$ **`kubectl run web-server --image registry.access.redhat.com/ubi8/httpd-24`**

You can use several options and flags with the `run` command. The `--command` option executes a custom command and its arguments in a container, rather than the default command that is defined in the container image. You must follow the `--command` option with a double dash (`--`) to separate the custom command and its arguments from the `run` command options. The following syntax is used with the `--command` option:

**``oc run _`RESOURCE/NAME`_ --image _`IMAGE`_ --command -- _`cmd`_ _`arg1`_ ... _`argN`_``**

You can also use the double dash option to provide custom arguments to a default command in the container image.

**``kubectl run _`RESOURCE/NAME`_ --image _`IMAGE`_ -- _`arg1`_ _`arg2`_ ... _`argN`_``**

To start an interactive session with a container in a pod, include the `-it` options before the pod name. The `-i` option tells Kubernetes to keep open the standard input (`stdin`) on the container in the pod. The `-t` option tells Kubernetes to open a TTY session for the container in the pod. You can use the `-it` options to start an interactive, remote shell in a container. From the remote shell, you can then execute additional commands in the container.

The following example starts an interactive remote shell, `/bin/bash`, in the default container in the `my-app` pod.

[user@host ~]$ **`oc run -it my-app --image registry.access.redhat.com/ubi9/ubi \ --command -- /bin/bash`**
If you don't see a command prompt, try pressing enter.
bash-5.1$

**Note

Unless you include the `--namespace` or `-n` options, the `run` command creates containers in pods in the current selected project.

You can also define a restart policy for containers in a pod by including the `--restart` option. A pod restart policy determines how the cluster should respond when containers in that pod exit. The `--restart` option has the following accepted values: `Always`, `OnFailure`, and `Never`.

`Always`

If the restart policy is set to `Always`, then the cluster continuously tries to restart a successfully exited container, for up to five minutes. The default pod restart policy is `Always`. If the `--restart` option is omitted, then the pod is configured with the `Always` policy.

`OnFailure`

Setting the pod restart policy to `OnFailure` tells the cluster to restart only failed containers in the pod, for up to five minutes.

`Never`

If the restart policy is set to `Never`, then the cluster does not try to restart exited or failed containers in a pod. Instead, the pods immediately fail and exit.

The following example command executes the `date` command in the container of the pod named `my-app`, redirects the `date` command output to the terminal, and defines `Never` as the pod restart policy.

[user@host ~]$ **`oc run -it my-app \ --image registry.access.redhat.com/ubi9/ubi \ --restart Never --command -- date`**
Mon Feb 20 22:36:55 UTC 2023

To automatically delete a pod after it exits, include the `--rm` option with the `run` command.

[user@host ~]$ **`kubectl run -it my-app --rm \ --image registry.access.redhat.com/ubi9/ubi \ --restart Never --command -- date`**
Mon Feb 20 22:38:50 UTC 2023
pod "date" deleted

For some containerized applications, you might need to specify environment variables for the application to work. To specify an environment variable and its value, include the `--env=` option with the `run` command.

[user@host ~]$ **`oc run mysql \ --image registry.redhat.io/rhel9/mysql-80 \ --env MYSQL_ROOT_PASSWORD=myP@$$123`**
pod/mysql created

### User and Group IDs Assignment

When a project is created, OpenShift adds annotations to the project that determine the user ID (UID) range and supplemental group ID (GID) for pods and their containers in the project. You can retrieve the annotations with the ``oc describe project _`project-name`_`` command.

[user@host ~]$ **`oc describe project my-app`**
Name:			my-app
_...output omitted..._
Annotations:   openshift.io/description=
			   openshift.io/display-name=
			   openshift.io/requester=developer
			   openshift.io/sa.scc.mcs=s0:c27,c4
			   **`openshift.io/sa.scc.supplemental-groups=1000710000/10000`**
			   **`openshift.io/sa.scc.uid-range=1000710000/10000`**
_...output omitted..._

With OpenShift default security policies, regular cluster users cannot choose the `USER` or UIDs for their containers. When a regular cluster user creates a pod, OpenShift ignores the `USER` instruction in the container image. Instead, OpenShift assigns to the user in the container a UID and a supplemental GID from the identified range in the project annotations. The GID of the user is always `0`, which means that the user belongs to the `root` group. Any files and directories that the container processes might write to must have read and write permissions by `GID=0` and have the `root` group as the owner. Although the user in the container belongs to the `root` group, the user is an unprivileged account.

In contrast, when a cluster administrator creates a pod, the `USER` instruction in the container image is processed. For example, if the `USER` instruction for the container image is set to `0`, then the user in the container is the `root` privileged account, with a `0` value for the UID. Executing a container as a privileged account is a security risk. A privileged account in a container has unrestricted access to the container's host system. Unrestricted access means that the container could modify or delete system files, install software, or otherwise compromise its host. Red Hat therefore recommends that you run containers as `rootless`, or as an unprivileged user with only the necessary privileges for the container to run.

Red Hat also recommends that you run containers from different applications with unique user IDs. Running containers from different applications with the same UID, even an unprivileged one, is a security risk. If the UID for two containers is the same, then the processes in one container could access the resources and files of the other container. By assigning a distinct range of UIDs and GIDs for each project, OpenShift ensures that applications in different projects do not run as the same UID or GID.

#### Pod Security

The Kubernetes Pod Security Admission controller issues a warning when a pod is created without a defined security context. Security contexts grant or deny OS-level privileges to pods. OpenShift uses the Security Context Constraints controller to provide safe defaults for pod security. You can ignore pod security warnings in these course exercises. Security Context Constraints (SCC) are discussed in more detail in course DO280: _Red Hat OpenShift Administration II: Operating a Production Kubernetes Cluster_.

### Execute Commands in Running Containers

To execute a command in a running container in a pod, you can use the `exec` command with the `kubectl` or `oc` CLI. The `exec` command uses the following syntax:

**``oc exec _`RESOURCE/NAME`_ -- _`COMMAND`_ [args...] [options]``**

The output of the executed command is sent to your terminal. In the following example, the `exec` command executes the `date` command in the `my-app` pod.

[user@host ~]$ **`oc exec my-app -- date`**
Tue Feb 21 20:43:53 UTC 2023

The specified command is executed in the first container of a pod. For multicontainer pods, include the `-c` or `--container=` options to specify which container is used to execute the command. The following example executes the `date` command in a container named `ruby-container` in the `my-app` pod.

[user@host ~]$ **`kubectl exec my-app -c ruby-container -- date`**
Tue Feb 21 20:46:50 UTC 2023

The `exec` command also accepts the `-i` and `-t` options to create an interactive session with a container in a pod. In the following example, Kubernetes sends `stdin` to the `bash` shell in the `ruby-container` container from the `my-app` pod, and sends `stdout` and `stderr` from the `bash` shell back to the terminal.

[user@host ~]$ **`oc exec my-app -c ruby-container -it -- bash -il`**
[1000780000@ruby-container /]$

In the previous example, a raw terminal is opened in the `ruby-container` container. From this interactive session, you can execute additional commands in the container. To terminate the interactive session, you must execute the `exit` command in the raw terminal.

[user@host ~]$ **`kubectl exec my-app -c ruby-container -it -- bash -il`**
[1000780000@ruby-container /]$ date
Tue Feb 21 21:16:00 UTC 2023
[1000780000@ruby-container] **`exit`**

### Container Logs

Container logs are the standard output (`stdout`) and standard error (`stderr`) output of a container. You can retrieve logs with the ``logs pod _`pod-name`_`` command that the `kubectl` and `oc` CLIs provide. The command includes the following options:

**`-l`** `or` **`--selector=''`**

Filter objects based on the specified `key:value` label constraint.

**`--tail=`**

Specify the number of lines of recent log files to display; the default value is `-1` with no selectors, which displays all log lines.

**`-c`** `or` **`--container=`**

Print the logs of a particular container in a multicontainer pod.

**`-f`** `or` **`--follow`**

Follow, or stream, logs for a container.

**`-p`** `or` **`--previous=true`**

Print the logs of a previous container instance in the pod, if it exists. This option is helpful for troubleshooting a pod that failed to start, because it prints the logs of the last attempt.

The following example restricts `oc logs` command output to the 10 most recent log files:

[user@host ~]$ **`oc logs postgresql-1-jw89j --tail=10`**
 done
server stopped
Starting server...
2023-01-04 22:00:16.945 UTC [1] LOG:  starting PostgreSQL 12.11 on x86_64-redhat-linux-gnu, compiled by gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10), 64-bit
2023-01-04 22:00:16.946 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
2023-01-04 22:00:16.946 UTC [1] LOG:  listening on IPv6 address "::", port 5432
2023-01-04 22:00:16.953 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
2023-01-04 22:00:16.960 UTC [1] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2023-01-04 22:00:16.968 UTC [1] LOG:  redirecting log output to logging collector process
2023-01-04 22:00:16.968 UTC [1] HINT:  Future log output will appear in directory "log".

You can also use the ``attach _`pod-name`_ -c _`container-name`_ -it`` command to connect to and start an interactive session on a running container in a pod. The ``-c _`container-name`_`` option is required for multicontainer pods. If the container name is omitted, then Kubernetes uses the `kubectl.kubernetes.io/default-container` annotation on the pod to select the container. Otherwise, the first container in the pod is chosen. You can use the interactive session to retrieve application log files and to troubleshoot application issues.

[user@host ~]$ **`oc attach my-app -it`**
If you don't see a command prompt, try pressing enter.

bash-4.4$

You can also retrieve logs from the web console by clicking the Logs tab of any pod.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/containers/assets/podlogs.png)|

If you have more than one container, then you can change between them to list the logs of each one.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/containers/assets/logscontainers.png)|

### Deleting Resources

You can delete Kubernetes resources, such as pod resources, with the `delete` command. The `delete` command can delete resources by resource type and name, resource type and label, standard input (`stdin`), and with JSON- or YAML-formatted files. The command accepts only one argument type at a time.

For example, you can supply the resource type and name as a command argument.

[user@host ~]$ **`oc delete pod php-app`**

You can also delete pods from the web console by clicking Actions and then Delete Pod in the pod's principal menu.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/containers/assets/podelete.png)|

To select resources based on labels, you can include the `-l` option and the `key:value` label as a command argument.

[user@host ~]$ **`kubectl delete pod -l app=my-app`**
pod "php-app" deleted
pod "mysql-db" deleted

You can also provide the resource type and a JSON- or YAML-formatted file that specifies the name of the resource. To use a file, you must include the `-f` option and provide the full path to the JSON- or YAML-formatted file.

[user@host ~]$ **`oc delete pod -f ~/php-app.json`**
pod "php-app" deleted

You can also use `stdin` and a JSON- or YAML-formatted file that includes the resource type and resource name with the `delete` command.

[user@host ~]$ **`cat ~/php-app.json | kubectl delete -f -`**
pod "php-app" deleted

Pods support graceful termination, which means that pods try to terminate their processes first before Kubernetes forcibly terminates the pods. To change the time period before a pod is forcibly terminated, you can include the `--grace-period` flag and a time period in seconds in your `delete` command. For example, to change the grace period to 10 seconds, use the following command:

[user@host ~]$ **`oc delete pod php-app --grace-period=10`**

To shut down the pod immediately, set the grace period to 1 second. You can also use the `--now` flag to set the grace period to 1 second.

[user@host ~]$ **`oc delete pod php-app --now`**

You can also forcibly delete a pod with the `--force` option. If you forcibly delete a pod, Kubernetes does not wait for a confirmation that the pod's processes ended, which can leave the pod's processes running until its node detects the deletion. Therefore, forcibly deleting a pod could result in inconsistency or data loss. Forcibly delete pods only if you are sure that the pod's processes are terminated.

[user@host ~]$ **`kubectl delete pod php-app --force`**

To delete all pods in a project, you can include the `--all` option.

[user@host ~]$ **`kubectl delete pods --all`**
pod "php-app" deleted
pod "mysql-db" deleted

Likewise, you can delete a project and its resources with the ``oc delete project _`project-name`_`` command.

[user@host ~]$ **`oc delete project my-app`**
project.project.openshift.io "my-app" deleted

### The CRI-O Container Engine

A container engine is required to run containers. Worker and control plane nodes in an OpenShift Container Platform cluster use the `CRI-O` container engine to run containers. Unlike tools such as Podman or Docker, the `CRI-O` container engine is a runtime that is designed and optimized specifically for running containers in a Kubernetes cluster. Because `CRI-O` meets the Kubernetes Container Runtime Interface (CRI) standards, the container engine can integrate with other Kubernetes and OpenShift tools, such as networking and storage plug-ins.

**Note

For more information about the Kubernetes Container Runtime Interface (CRI) standards, refer to the _CRI-API_ repository at [https://github.com/kubernetes/cri-api](https://github.com/kubernetes/cri-api).

`CRI-O` provides a command-line interface to manage containers with the `crictl` command. The `crictl` command includes several subcommands to help you to manage containers. The following subcommands are commonly used with the `crictl` command:

**`crictl pods`**

Lists all pods on a node.

**`crictl image`**

Lists all images on a node.

**`crictl inspect`**

Retrieve the status of one or more containers.

**`crictl exec`**

Run a command in a running container.

**`crictl logs`**

Retrieve the logs of a container.

**`crictl ps`**

List running containers on a node.

To manage containers with the `crictl` command, you must first identify the node that is hosting your containers.

[user@host ~]$ **`kubectl get pods -o wide`**
NAME                READY STATUS    RESTARTS AGE IP        NODE
postgresql-1-8lzf2  1/1   Running   0        20m 10.8.0.64 **`master01`**
postgresql-1-deploy 0/1   Completed 0        21m 10.8.0.63 master01

[user@host ~]$ **`oc get pod postgresql-1-8lzf2 -o jsonpath='{.spec.nodeName}{"\n"}'`**
master01

Next, you must connect to the identified node as a cluster administrator. Cluster administrators can use SSH to connect to a node or create a debug pod for the node. Regular users cannot connect to or create debug pods for cluster nodes.

As a cluster administrator, you can create a debug pod for a node with the ``oc debug node/_`node-name`_`` command. OpenShift creates the ``pod/_`node-name`_-debug`` pod in your currently selected project and automatically connects you to the pod. You must then enable access host binaries, such as the `crictl` command, with the `chroot /host` command. This command mounts the host's root file system in the `/host` directory within the debug pod shell. By changing the root directory to the `/host` directory, you can run binaries contained in the host's executable path.

[user@host ~]$ **`oc debug node/master01`**
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.
sh-4.4# **`chroot /host`**

After enabling host binaries, you can use the `crictl` command to manage the containers on the node. For example, you can use the `crictl ps` and `crictl inspect` commands to retrieve the process ID (`PID`) of a running container. You can then use the PID to retrieve or enter the namespaces within a container, which is useful for troubleshooting application issues.

To find the PID of a running container, you must first determine the container's ID. You can use the `crictl ps` command with the `--name` option to filter the command output to a specific container.

sh-5.1# **`crictl ps --name postgresql`**
CONTAINER     IMAGE        CREATED STATE   NAME       ATTEMPT POD ID      POD
27943ae4f3024 image...7104 5...ago Running postgresql 0       5768...f015 postgresql-1...

The default output of the `crictl ps` command is a table. You can find the short container ID under the `CONTAINER` column. You can also use the `-o` or `--output` options to specify the format of the `crictl ps` command as JSON or YAML and then parse the output. The parsed output displays the full container ID.

sh-5.1# **`crictl ps --name postgresql -o json | jq .containers[0].id`**
"2794...29a4"

After identifying the container ID, you can use the `crictl inspect` command and the container ID to retrieve the PID of the running container. By default, the `crictl inspect` command displays verbose output. You can use the `-o` or `--output` options to format the command output as JSON, YAML, a table, or as a Go template. If you specify the JSON format, you can then parse the output with the `jq` command. Likewise, you can use the `grep` command to limit the command output.

sh-5.1# **`crictl inspect -o json 27943ae4f3024 | jq .info.pid`**
**`43453`**
sh-5.1# **`crictl inspect 27943ae4f3024 | grep pid`**
    "pid": **`43453`**,
_...output omitted..._

After determining the PID of a running container, you can use the ``lsns -p _`PID`_`` command to list the system namespaces of a container.

sh-5.1# **`lsns -p 43453`**
        NS TYPE   NPROCS   PID USER       COMMAND
4026531835 cgroup    530     1 root       /usr/lib/systemd/systemd --switched-root --system --deserialize 17
4026531837 user      530     1 root       /usr/lib/systemd/systemd --switched-root --system --deserialize 17
4026537853 uts         8 43453 1000690000 postgres
4026537854 ipc         8 43453 1000690000 postgres
4026537856 net         8 43453 1000690000 postgres
4026538013 mnt         8 43453 1000690000 postgres
4026538014 pid         8 43453 1000690000 postgres

You can also use the PID of a running container with the `nsenter` command to enter a specific namespace of a running container. For example, you can use the `nsenter` command to execute a command within a specified namespace on a running container. The following example executes the `ps -ef` command within the process namespace of a running container.

sh-5.1# **`nsenter -t 43453 -p -r ps -ef`**
UID          PID    PPID  C STIME TTY          TIME CMD
1000690+       1       0  0 18:49 ?        00:00:00 postgres
1000690+      58       1  0 18:49 ?        00:00:00 postgres: logger
1000690+      60       1  0 18:49 ?        00:00:00 postgres: checkpointer
1000690+      61       1  0 18:49 ?        00:00:00 postgres: background writer
1000690+      62       1  0 18:49 ?        00:00:00 postgres: walwriter
1000690+      63       1  0 18:49 ?        00:00:00 postgres: autovacuum launcher
1000690+      64       1  0 18:49 ?        00:00:00 postgres: stats collector
1000690+      65       1  0 18:49 ?        00:00:00 postgres: logical replication launcher
root        7414       0  0 20:14 ?        00:00:00 ps -ef

The `-t` option specifies the PID of the running container as the target PID for the `nsenter` command. The `-p` option directs the `nsenter` command to enter the process or `pid` namespace. The `-r` option sets the top-level directory of the process namespace as the root directory, thus enabling commands to execute in the context of the namespace.

You can also use the `-a` option to execute a command in all of the container's namespaces.

sh-5.1# **`nsenter -t 43453 -a ps -ef`**
UID          PID    PPID  C STIME TTY          TIME CMD
1000690+       1       0  0 18:49 ?        00:00:00 postgres
1000690+      58       1  0 18:49 ?        00:00:00 postgres: logger
1000690+      60       1  0 18:49 ?        00:00:00 postgres: checkpointer
1000690+      61       1  0 18:49 ?        00:00:00 postgres: background writer
1000690+      62       1  0 18:49 ?        00:00:00 postgres: walwriter
1000690+      63       1  0 18:49 ?        00:00:00 postgres: autovacuum launcher
1000690+      64       1  0 18:49 ?        00:00:00 postgres: stats collector
1000690+      65       1  0 18:49 ?        00:00:00 postgres: logical replication launcher
root       10058       0  0 20:45 ?        00:00:00 ps -ef

## Find and Inspect Container Images

### Objectives

- Find containerized applications in container registries and get information about the runtime parameters of supported and community container images.
    

### Container Image Overview

A container is an isolated runtime environment where applications are executed as isolated processes. The isolation of the runtime environment ensures that applications do not interfere with other containers or system processes.

A container image contains a packaged version of your application, with all the necessary dependencies for the application to run. Images can exist without containers. However, containers depend on images, because containers use container images to build a runtime environment to execute applications.

Containers can be split into two similar but distinct concepts: _container images_ and _container instances_. A _container image_ contains immutable data that defines an application and its libraries. You can use container images to create _container instances_, which are running processes that are isolated by a set of kernel namespaces.

You can use each container image many times to create many distinct container instances. These replicas can be split across multiple hosts. The application within a container is independent of the host environment.

### Container Image Registries

Image registries are services that offer container images to download. Image creators and maintainers can store and distribute container images in a controlled manner to public or private audiences. Some examples of image registries include Quay.io, Red Hat Registry, Docker Hub, and Amazon ECR.

#### Red Hat Registry

Red Hat distributes container images by using two registries: `registry.access.redhat.com` (where no authentication is required), and `registry.redhat.io` (where authentication is required). The Red Hat Ecosystem Catalog, [https://catalog.redhat.com/](https://catalog.redhat.com/), provides centralized searching utility for both registries. You can search the Red Hat Ecosystem Catalog for technical details about container images. The catalog hosts a large set of container images, including from major open source projects, such as Apache, MySQL, and Jenkins.

Because the Red Hat Ecosystem Catalog is also searched for software products other than container images, you must navigate to `[https://catalog.redhat.com/software/containers/explore](https://catalog.redhat.com/software/containers/explore)` to specifically search for container images.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/images/assets/redhatcatalog.png)|

Figure 3.4: Red Hat Ecosystem Catalog

The details page of a container image gives relevant information, such as technical data, the installed packages within the image, or a security scan. You can navigate through these options by using the tabs on the website. You can also change the image version by selecting a specific tag.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/images/assets/redhatcatalog2.png)|

Figure 3.5: The Red Hat Universal Base Image 9

The Red Hat internal security team vets all images in the container catalog. Red Hat rebuilds all components to avoid known security vulnerabilities.

Red Hat container images provide the following benefits:

- Trusted source: All container images use sources that Red Hat knows and trusts.
    
- Original dependencies: None of the container packages are tampered with, and include only known libraries.
    
- Vulnerability-free: Container images are free of known critical vulnerabilities in the platform components or layers.
    
- Runtime protection: All applications in container images run as non-root users, to minimize the exposure surface to malicious or faulty applications.
    
- Red Hat Enterprise Linux (RHEL) compatible: Container images are compatible with all RHEL platforms, from bare metal to cloud.
    
- Red Hat support: Red Hat commercially supports the complete stack.
    

**Note

You must log in to the `registry.redhat.io` registry with a customer portal account or a Red Hat Developer account to use the stored container images in the registry.

#### Quay.io

Although the Red Hat Registry stores only images from Red Hat and certified providers, you can store your own images with Quay.io, another public image registry that Red Hat sponsors. Although storing public images in Quay is free of charge, some options are available only for paying customers. Quay also offers an on-premise version of the product, which you can use to set up an image registry in your own servers.

Quay.io introduces features such as server-side image building, fine-grained access controls, and automatic scanning of images for known vulnerabilities.

Quay.io offers live images that creators regularly update. Quay.io users can create their namespaces, with fine-grained access control, and publish their created images to that namespace. Container Catalog users rarely or never push new images, but consume trusted images from the Red Hat team.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/images/assets/quay.png)|

Figure 3.6: The Quay.io welcome page

#### Private Registries

Image creators or maintainers might want to make their images publicly available. However, other image creators might prefer to keep their images private, for the following reasons:

- Company privacy and secret protection
    
- Legal restrictions and laws
    
- Avoidance of publishing images in development
    

In some cases, private images are preferred. Private registries give image creators control over image placement, distribution, and usage. Private images are more secure than images in public registries.

#### Public Registries

Other public registries, such as Docker Hub and Amazon ECR, are also available for storing, sharing, and consuming container images. These registries can include official images that the registry owners or the registry community users create and maintain. For example, Docker Hub hosts a Docker Official Image of a WordPress container image. Although the `docker.io/library/wordpress` container image is a Docker Official Image, the container image is not supported by WordPress, Docker, or Red Hat. Instead, the Docker Community, a global group of Docker Hub users, supports and maintains the container image. Support for this container image depends on the availability and skills of the Docker Community users.

Consuming container images from public registries brings risks. For example, a container image might include malicious code or vulnerabilities, which can compromise the host system that executes the container image. A host system can also be compromised by public container images, because the images are often configured with the privileged `root` user. Additionally, the software in a container image might not be correctly licensed, or might violate licensing terms.

Before you use a container image from a public registry, review and verify the container image. Also ensure that you have the correct permissions to use the software in the container image.

### Container Image Identifiers

Several objects provide identifying information about a container image.

**`Registry`**

It is a content server, such as `registry.access.redhat.com`, that is used to store and share container images. A registry consists of one or more repositories that contain tagged container images.

**`Name`**

It identifies the container image repository; it is a string that is composed of letters, numbers, and some special characters. This component refers to the name of the directory, or the container repository, within the container registry where the container image is.

For example, consider the fully qualified domain name (FQDN) of the `registry.access.redhat.com/ubi9/httpd-24:1-233` container image. The container image is in the `ubi9/httpd-24` repository in the `registry.access.redhat.com` container registry.

**`ID/Hash`**

It is the SHA (Secure Hash Algorithm) code to pull or verify an image. The SHA image ID cannot change, and always references the same container image content. The ID/hash is the true, unique identifier of an image. For example, the `sha256:4186a1ead13fc30796f951694c494e7630b82c320b81e20c020b3b07c888985b` image ID always refers to the `registry.access.redhat.com/ubi9/httpd-24:1-233` container image.

**`Tag`**

It is a label for a container image in a repository, to distinguish from other images, for version control. The `tag` comes after the image repository name and is delimited by a colon (:).

When an image tag is omitted, the floating tag, `latest`, is used as the default tag. A floating tag is an alias to another tag. In contrast, a fixed tag points to a specific container build. For the `registry.access.redhat.com/ubi9/httpd-24:1-233.1669634588` container image, `1-233.1669634588` is the fixed tag for the image, and at the time of writing, corresponds to the floating `latest` flag.

### Container Image Components

A container image is composed of multiple components.

**`Layers`**

Container images are created from instructions. Each instruction adds a layer to the container image. Each layer consists of the differences between it and the following layer. The layers are then stacked to create a read-only container image.

**`Metadata`**

Metadata includes the instructions and documentation for a container image.

### Container Image Instructions and Metadata

Container image layers consist of instructions, or steps, and metadata for building the image. You can override instructions during container creation to adjust the container image according to your needs. Some instructions can affect the running container, and other instructions are for informational purposes only.

The following instructions affect the state of a running container:

**`ENV`**

Defines the available environment variables in the container. A container image might include multiple `ENV` instructions. Any container can recognize additional environment variables that are not listed in its metadata.

**`ARG`**

It defines build-time variables, typically to make a customizable container build. Developers commonly configure the `ENV` instructions by using the `ARG` instruction. It is useful for preserving the build-time variables for run time.

**`USER`**

Defines the active user in the container. Later instructions run as this user. It is a good practice to define a user other than `root` for security purposes. OpenShift does not honor the user in a container image, for regular cluster users. Only cluster administrators can run containers (pods) with their chosen _user ID (UIDs)_ and _group IDs (GIDs)_.

**`ENTRYPOINT`**

It defines the executable to run when the container is started.

**`CMD`**

It defines the command to execute when the container is started. This command is passed to the executable that the `ENTRYPOINT` instruction defines. Base images define a default `ENTRYPOINT` executable, which is usually a shell executable, such as Bash.

**`WORKDIR`**

It sets the current working directory within the container. Later instructions execute within this directory.

Metadata is used for documentation purposes, and does not affect the state of a running container. You can also override the metadata values during container creation.

The following metadata is for information only, and does not affect the state of the running container:

**`EXPOSE`**

It indicates the network port that the application binds to within the container. This metadata does not automatically bind the port on the host, and is used only for documentation purposes.

**`VOLUME`**

It defines where to store data outside the container. The value shows the path where your container runtime mounts the directory inside the container. More than one path can be defined to create multiple volumes.

**`LABEL`**

Adds a key-value pair to the metadata of the image for organization and image selection.

Container engines are not required to honor metadata in a container image, such as `USER` or `EXPOSE`. A container engine can also recognize additional environment variables that are not listed in the container image metadata.

### Base Images

A _base image_ is the image that your resulting container image is built on. Your chosen base image determines the Linux distribution, and any of the following components:

- Package manager
    
- `Init` system
    
- File system layout
    
- Preinstalled dependencies and runtimes
    

The base image can also influence factors such as image size, vendor support, and processor compatibility.

Red Hat provides enterprise-grade container images that are engineered to be the base operating system layer for your containerized applications. These container images are intended as a common starting point for containers, and are known as _universal base images_ (UBI). Red Hat UBI container images are _Open Container Initiative (OCI)_ compliant images that contain portions of Red Hat Enterprise Linux (RHEL). UBI container images include a subset of RHEL content. They provide a set of prebuilt runtime languages, such as Python and Node.js, and associated DNF repositories that you can use to add application dependencies. UBI-based images can be distributed without cost or restriction. They can be deployed to both Red Hat and non-Red Hat platforms, and be pushed to your chosen container registry.

A Red Hat subscription is not required to use or distribute UBI-based images. However, Red Hat provides full support only for containers that are built on UBI if the containers are deployed to a Red Hat platform, such as a Red Hat OpenShift Container Platform (RHOCP) cluster or RHEL.

Red Hat provides four UBI variants: `standard`, `init`, `minimal`, and `micro`. All UBI variants and UBI-based images use Red Hat Enterprise Linux (RHEL) at their core and are available from the Red Hat Container Catalog. The main differences are as follows:

Standard

This image is the primary UBI, which includes DNF, systemd, and utilities such as `gzip` and `tar`.

Init

This image simplifies running multiple applications within a single container by managing them with systemd.

Minimal

This image is smaller than the `init` image and provides nice-to-have features. This image uses the `microdnf` minimal package manager instead of the full-sized version of DNF.

Micro

This image is the smallest available UBI, and includes only the minimum packages. For example, this image does not include a package manager.

### Inspecting and Managing Container Images

Various tools can inspect and manage container images, including the `oc image` command and Skopeo.

#### Skopeo

Skopeo is another tool to inspect and manage remote container images. With Skopeo, you can copy and sync container images from different container registries and repositories. You can also copy an image from a remote repository and save it to a local disk. If you have the appropriate repository permissions, then you can also delete an image from container registry. You also can use Skopeo to inspect the configuration and contents of a container image, and to list the available tags for a container image. Unlike other container image tools, Skopeo can execute without a privileged account, such as `root`. Skopeo does not require a running daemon to execute various operations.

Skopeo is executed with the `skopeo` command-line utility, which you can install with various package managers, such as DNF, Brew, and APT. The `skopeo` utility might already be installed on some Linux-based distributions. You can install the `skopeo` utility on Fedora, CentOS Stream 8 and later, and Red Hat Enterprise Linux 8 and later systems by using the DNF package manager.

[user@host ~]$ **`sudo dnf -y install skopeo`**

The `skopeo` utility is currently not available as a packaged binary for Windows-based systems. However, the `skopeo` utility is available as a container image from the `quay.io/skopeo/stable` container repository. For more information about the Skopeo container image, refer to the `skopeoimage` overview guide in the Skopeo repository (`[https://github.com/containers/skopeo/blob/main/contrib/skopeoimage/README.md](https://github.com/containers/skopeo/blob/main/contrib/skopeoimage/README.md)`).

You can also build `skopeo` from source code in a container, or build it locally without using a container. Refer to the installation guide in the Skopeo repository (`[https://github.com/containers/skopeo/blob/main/install.md#container-images](https://github.com/containers/skopeo/blob/main/install.md#container-images)`) for more information about installing or building Skopeo from source code.

The `skopeo` utility provides commands to help you to manage and inspect container images and container image registries. For container registries that require authentication, you must first log in to the registry before you can execute additional `skopeo` commands.

[user@host ~]$ **`skopeo login quay.io`**

**Note

OpenShift clusters are typically configured with registry credentials. When a pod is created from a container image in a remote repository, OpenShift authenticates to the container registry with the configured registry credentials, and then pulls, or copies, the image. Because OpenShift automatically uses the registry credentials, you typically do not need to manually authenticate to a container registry when you create a pod. By contrast, the `oc image` command and the `skopeo` utility require you first to log in to a container registry.

After you log in to a container registry (if required), you can execute additional `skopeo` commands against container images in a repository. When you execute a `skopeo` command, you must specify the transport and the repository name. A _transport_ is the mechanism to transfer or move container images between locations. Two common transports are `docker` and `dir`. The `docker` transport is used for container registries, and the `dir` transport is used for local directories.

The `oc image` command and other tools default to the `docker` transport, and so you do not need to specify the transport when executing commands. However, the `skopeo` utility does not define a default transport; you must specify the transport with the container image name. Most `skopeo` commands use the ``skopeo _`command`_ _`[command options]`_ _`transport`_://_`IMAGE-NAME`_`` format. For example, the following `skopeo list-tags` command lists all available tags in a `registry.access.redhat.com/ubi9/httpd-24` container repository by using the `docker` transport:

[user@host ~]$ **`skopeo list-tags docker://registry.access.redhat.com/ubi9/httpd-24`**
{
    "Repository": "registry.access.redhat.com/ubi9/httpd-24",
    "Tags": [
        "1-229",
        "1-217.1666632462",
        "1-201",
        "1-194.1655192191-source",
        "1-229-source",
        "1-194-source",
        "1-201-source",
        "1-217.1664813224",
        "1-217.1664813224-source",
        "1-210-source",
        "1-233.1669634588",
        "1",
        "1-217.1666632462-source",
        "1-233",
        "1-194.1655192191",
        "1-217.1665076049-source",
        "1-217",
        "1-233.1669634588-source",
        "1-210",
        "1-217.1665076049",
        "1-217-source",
        "1-233-source",
        "1-194",
        "latest"
    ]
}

The `skopeo` utility includes other useful commands for container image management.

**`skopeo inspect`**

View low-level information for an image name, such as environment variables and available tags. Use the ``skopeo inspect _`[command options]`_ _`transport`_://_`IMAGE-NAME`_`` command format. You can include the `--config` flag to view the configuration, metadata, and history of a container repository. The following example retrieves the configuration information for the `registry.access.redhat.com/ubi9/httpd-24` container repository:

[user@host ~]$ **`skopeo inspect --config docker://registry.access.redhat.com/ubi9/httpd-24`**
_...output omitted..._
    "config": {
        "User": "1001",
        "ExposedPorts": {
            "8080/tcp": {},
            "8443/tcp": {}
        },
        "Env": [
_...output omitted..._
            "HTTPD_MAIN_CONF_PATH=/etc/httpd/conf",
            "HTTPD_MAIN_CONF_MODULES_D_PATH=/etc/httpd/conf.modules.d",
            "HTTPD_MAIN_CONF_D_PATH=/etc/httpd/conf.d",
            "HTTPD_TLS_CERT_PATH=/etc/httpd/tls",
            "HTTPD_VAR_RUN=/var/run/httpd",
            "HTTPD_DATA_PATH=/var/www",
            "HTTPD_DATA_ORIG_PATH=/var/www",
            "HTTPD_LOG_PATH=/var/log/httpd"
        ],
        "Entrypoint": [
            "container-entrypoint"
        ],
        "Cmd": [
            "/usr/bin/run-httpd"
        ],
        "WorkingDir": "/opt/app-root/src",
_...output omitted..._
    }
_...output omitted..._

**`skopeo copy`**

Copy an image from one location or repository to another. Use the ``skopeo copy _`transport`_://_`SOURCE-IMAGE`_ _`transport`_://_`DESTINATION-IMAGE`_`` format. For example, the following command copies the `quay.io/skopeo/stable:latest` container image to the `skopeo` repository in the `registry.example.com` container registry:

[user@host ~]$ **`skopeo copy docker://quay.io/skopeo/stable:latest \ docker://registry.example.com/skopeo:latest`**

**`skopeo delete`**

Delete a container image from a repository. You must use the ``skopeo delete _`[command options]`_ _`transport`_://_`IMAGE-NAME`_`` format. The following command deletes the `skopeo:latest` image from the `registry.example.com` container registry:

[user@host ~]$ **`skopeo delete docker://registry.example.com/skopeo:latest`**

**`skopeo sync`**

Synchronize one or more images from one location to another. Use this command to copy all container images from a source to a destination. The command uses the ``skopeo sync _`[command options]`_ --src _`transport`_ --dest _`transport`_ _`SOURCE`_ _`DESTINATION`_`` format. The following command synchronizes the `registry.access.redhat.com/ubi8/httpd-24` container repository to the `registry.example.com/httpd-24` container repository:

[user@host ~]$ **`skopeo sync --src docker --dest docker \ registry.access.redhat.com/ubi8/httpd-24 registry.example.com/httpd-24`**

#### Registry Credentials

Some registries require users to authenticate. For example, Red Hat containers that are based on RHEL typically require authenticated access:

[user@host ~]$ **`skopeo inspect docker://registry.redhat.io/rhel8/httpd-24`**
FATA[0000] Error parsing image name "docker://registry.redhat.io/rhel8/httpd-24": **`unable to retrieve auth token: invalid username/password: unauthorized: Please login to the Red Hat Registry using your Customer Portal credentials.`** Further instructions can be found here: https://access.redhat.com/RegistryAuthentication

You might choose a different image that does not require authentication, such as the UBI 8 image:

[user@host ~]$ **`skopeo inspect docker://registry.access.redhat.com/ubi8:latest`**
{
    "Name": "registry.access.redhat.com/ubi8",
    "Digest": "sha256:70fc...1173",
    "RepoTags": [
        "8.7-1054-source",
        "8.6-990-source",
        "8.6-754",
        "8.4-203.1622660121-source",
_...output omitted..._.

Alternatively, you must execute the `skopeo login` command for the registry before you can access the RHEL 8 image.

[user@host ~]$ **`skopeo login registry.redhat.io`**
Username: **`YOUR_USER`**
Password: **`YOUR_PASSWORD`**
Login Succeeded!
[user@host ~]$ **`skopeo list-tags docker://registry.redhat.io/rhel8/httpd-24`**
{
    "Repository": "registry.redhat.io/rhel8/httpd-24",
    "Tags": [
        "1-166.1645816922",
        "1-209",
        "1-160-source",
        "1-112",
_...output omitted..._

Skopeo stores the credentials in the `${XDG_RUNTIME_DIR}/containers/auth.json` file, where the `${XDG_RUNTIME_DIR}` refers to a directory that is specific to the current user. The credentials are encoded in the base64 format:

[user@host ~]$ **`cat ${XDG_RUNTIME_DIR}/containers/auth.json`**
{
	"auths": {
		"registry.redhat.io": {
			"auth": **`"dXNlcjpodW50ZXIy"`**
		}
	}
}
[user@host ~]$ **`echo -n dXNlcjpodW50ZXIy | base64 -d`**
user:hunter2

**Note

For security reasons, the `skopeo login` command does not show your password in the interactive session. Although you do not see what you are typing, Skopeo registers every key stroke. After typing your full password in the interactive session, press **Enter** to start the login.

#### The `oc image` Command

The OpenShift command-line interface provides the `oc image` command. You can use this command to inspect, configure, and retrieve information about container images.

The `oc image info` command inspects and retrieves information about a container image. You can use the `oc image info` command to identify the ID/hash SHA and to list the image layers of a container image. You can also review container image metadata, such as environment variables, network ports, and commands. If a container image repository provides a container image in multiple architectures, such as `amd64` or `arm64`, then you must include the `--filter-by-os` tag. For example, you can execute the following command to retrieve information about the `registry.access.redhat.com/ubi9/httpd-24:1-233` container image that is based on the `amd64` architecture:

[user@host ~]$ **`oc image info registry.access.redhat.com/ubi9/httpd-24:1-233 \ --filter-by-os amd64`**
Name:          registry.access.redhat.com/ubi9/httpd-24:1-233
Digest:        sha256:4186...985b
_...output omitted..._
Image Size:    130.8MB in 3 layers
Layers:        79.12MB sha256:d74e...1cad
               17.32MB sha256:dac0...a283
               34.39MB sha256:47d8...5550
OS:            linux
Arch:          amd64
Entrypoint:    container-entrypoint
Command:       /usr/bin/run-httpd
Working Dir:   /opt/app-root/src
User:          1001
Exposes Ports: 8080/tcp, 8443/tcp
Environment:   container=oci
_...output omitted..._
               HTTPD_CONTAINER_SCRIPTS_PATH=/usr/share/container-scripts/httpd/
               HTTPD_APP_ROOT=/opt/app-root
               HTTPD_CONFIGURATION_PATH=/opt/app-root/etc/httpd.d
               HTTPD_MAIN_CONF_PATH=/etc/httpd/conf
               HTTPD_MAIN_CONF_MODULES_D_PATH=/etc/httpd/conf.modules.d
               HTTPD_MAIN_CONF_D_PATH=/etc/httpd/conf.d
               HTTPD_TLS_CERT_PATH=/etc/httpd/tls
               HTTPD_VAR_RUN=/var/run/httpd
               HTTPD_DATA_PATH=/var/www
               HTTPD_DATA_ORIG_PATH=/var/www
               HTTPD_LOG_PATH=/var/log/httpd
_...output omitted..._

The `oc image` command provides more options to manage container images.

**`oc image append`**

Use this command to add layers to container images, and then push the container image to a registry.

**`oc image extract`**

You can use this command to extract or copy files from a container image to a local disk. Use this command to access the contents of a container image without first running the image as a container. A running container engine is not required.

**`oc image mirror`**

Copy or mirror container images from one container registry or repository to another. For example, you can use this command to mirror container images between public and private registries. You can also use this command to copy a container image from a registry to a disk. The command mirrors the HTTP structure of a container registry to a directory on a disk. The directory on the disk can then be served as a container registry.

### Running Containers as Root

Running containers as the root user is a security risk, because an attacker could exploit the application, access the container, and exploit further vulnerabilities to escape from the containerized environment into the host system. Attackers might escape the containerized environment by exploiting bugs and vulnerabilities that are typically in the kernel or container runtime.

Traditionally, when an attacker gains access to the container file system by using an exploit, the root user inside the container corresponds to the root user outside the container. If an attacker escapes the container isolation, then they have elevated privileges on the host system, which potentially causes more damage.

Containers that do not run as the root user have limitations that might prove unsuitable for use in your application, such as the following limitations:

Non-trivial Containerization

Some applications might require the root user. Depending on the application architecture, some applications might not be suitable for non-root containers, or might require a deeper understanding to containerize.

For example, applications such as HTTPd and Nginx start a bootstrap process and then create a process with a non-privileged user, which interacts with external users. Such applications are non-trivial to containerize for rootless use.

Red Hat provides containerized versions of HTTPd and Nginx that do not require root privileges for production usage. You can find the containers in the Red Hat container registry (https://catalog.redhat.com/software/containers/explore).

Required Use of Privileged Utilities

Non-root containers cannot bind to privileged ports, such as the `80` or `443` ports. Red Hat advises against using privileged ports, but to use port forwarding instead.

Similarly, non-root containers cannot use the `ping` utility by default, because it requires elevated privileges to establish raw sockets.

## Troubleshoot Containers and Pods

### Objectives

- Troubleshoot a pod by starting additional processes on its containers, changing their ephemeral file systems, and opening short-lived network tunnels.
    

### Container Troubleshooting Overview

Containers are designed to be immutable and ephemeral. A running container must be redeployed when changes are needed or when a new container image is available. However, you can change a running container without redeployment.

Updating a running container is best reserved for troubleshooting problematic containers. Red Hat does not generally recommend editing a running container to fix errors in a deployment. Changes to a running container are not captured in source control, but help to identify the needed corrections to the source code for the container functions. Capture these container updates in version control after you identify the necessary changes. Then, build a new container image and redeploy the application.

Custom alterations to a running container are incompatible with elegant architecture, reliability, and resilience for the environment.

### CLI Troubleshooting Tools

Administrators use various tools to interact with, inspect, and alter running containers. Administrators can use commands such as `oc get` to gather initial details for a specified resource type. Other commands are available for detailed inspection of a resource, or to update a resource in real time.

**Note

When interacting with the cluster containers, take suitable precautions with actively running components, services, and applications.

Use these tools to validate the functions and environment for a running container:

- The `kubectl` CLI provides the following commands:
    
    - `kubectl describe`: Display the details of a resource.
        
    - `kubectl edit`: Edit a resource configuration by using the system editor.
        
    - `kubectl patch`: Update a specific attribute or field for a resource.
        
    - `kubectl replace`: Deploy a new instance of the resource.
        
    - `kubectl cp`: Copy files and directories to and from containers.
        
    - `kubectl exec`: Execute a command within a specified container.
        
    - `kubectl explain`: Display documentation for a specified resource.
        
    - `kubectl port-forward`: Configure a port forwarder for a specified container.
        
    - `kubectl logs`: Retrieve the logs for a specified container.
        
    

Besides supporting the previous `kubectl` commands, the `oc` CLI adds the following commands for inspecting and troubleshooting running containers:

- The `oc` CLI provides the following commands:
    
    - `oc status`: Display the status of the containers in the selected namespace.
        
    - `oc rsync`: Synchronize files and directories to and from containers.
        
    - `oc rsh`: Start a remote shell within a specified container.
        
    

### Editing Resources

Troubleshooting and remediation often begin with a phase of inspection and data gathering. When solving issues, the `describe` command can provide helpful details about the running resource, such as the definition of a container and its purpose.

The following example demonstrates use of the ``oc describe _`RESOURCE`_ _`NAME`_`` command to retrieve information about a pod in the `openshift-dns` namespace:

[user@host ~]$ **`oc describe pod dns-default-lt13h`**
Name:               dns-default-lt13h
Namespace:          openshift-dns
Priority:           2000001000
Priority Class Name: system-node-critical
_...output omitted..._

Various CLI tools can apply a change that you determine is needed to a running container. The `edit` command opens the specified resource in the default editor for your environment. This editor is specified by setting either the `KUBE_EDITOR` or the `EDITOR` environment variable, or otherwise with the vi editor in Linux or the Notepad application in Windows.

The following example demonstrates use of the ``oc edit _`RESOURCE`_ _`NAME`_`` command to edit a running container:

[user@host ~]$ **`oc edit pod mongo-app-sw88b`**

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Pod
metadata:
  annotations:
_...output omitted..._

You can also use the `patch` command to update fields of a resource.

The following example uses the `patch` command to update the container image that a pod uses:

[user@host ~]$ **`oc patch pod valid-pod --type='json' \ -p='[{"op": "replace", "path": "/spec/containers/0/image", \ "value":"http://registry.access.redhat.com/ubi8/httpd-24"}]'`**

**Note

For more information about patching resources and the different merge methods, refer to [Update API Objects in Place Using kubectl patch](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/).

### Copy Files to and from Containers

Administrators can copy files and directories to or from a container to inspect, update, or correct functionality. Adding a configuration file or retrieving an application log are common use cases.

**Note

To use the `cp` command with the `kubectl` CLI or the `oc` CLI, the `tar` binary must be present in the container. If the binary is absent, then an error message appears and the operation fails.

The following example demonstrates copying a file from a running container to a local directory by using the ``oc cp _`SOURCE`_ _`DEST`_`` command:

[user@host ~]$ **`oc cp apache-app-kc82c:/var/www/html/index.html /tmp/index.bak`**

[user@host ~]$ **`ls /tmp`**
index.bak

The following example demonstrates use of the ``oc cp _`SOURCE`_ _`DEST`_`` command to copy a file from a local directory to a directory in a running container:

[user@host ~]$ **`oc cp /tmp/index.html apache-app-kc82c:/var/www/html/`**

[user@host ~]$ **`oc exec -it apache-app-kc82c -- ls /var/www/html`**
index.html

**Note

Targeting a file path within a pod for either the ``_`SOURCE`_`` or ``_`DEST`_`` argument uses the ``_`pod_name:path`_`` format, and can include the ``-c _`container_name`_`` option to specify a container within the pod. If you omit the ``-c _`container_name`_`` option, then the command targets the first container in the pod.

Additionally, when using the `oc` CLI, file and directory synchronization is available by using the `oc rsync` command.

The following example demonstrates use of the ``oc rsync _`SOURCE_NAME`_ _`DEST`_`` command to synchronize files from a running container to a local directory.

[user@host ~]$ **`oc rsync apache-app-kc82c:/var/www/ /tmp/web_files`**

[user@host ~]$ **`ls /tmp/web_files`**
cgi-bin
html

The `oc rsync` command uses the `rsync` client on your local system to copy changed files to and from a pod container. The `rsync` binary must be available locally and within the container for this approach. If the `rsync` binary is not found, then a `tar` archive is created on the local system and is sent to the container. The container then uses the `tar` utility to extract files from the archive. Without the `rsync` and `tar` binaries, an error message occurs and the `oc rsync` command fails.

**Note

For Linux-based systems, you can install the `rsync` client and the `tar` utility on a local system by using a package manager, such as DNF. For Windows-based systems, you can install the `cwRsync` client. For more information about the `cwRysnc` client, refer to [https://www.itefix.net/cwrsync](https://www.itefix.net/cwrsync).

### Remote Container Access

Exposing a network port for a container is routine, especially for containers that provide a service. In a cluster, port forwarding connections are made through the kubelet, which maps a local port on your system to a port on a pod. Configuring port forwarding creates a request through the Kubernetes API, and creates a multiplexed stream, such as HTTP/2, with a `port` header that specifies the target port in the pod. The kubelet delivers the stream data to the target pod and port, and vice versa for egress data from the pod.

When troubleshooting an application that typically runs without a need to connect locally, you can use the port-forwarding function to expose connectivity to the pod for investigation. With this function, an administrator can connect on the new port and inspect the problematic application. After you remediate the issue, the application can be redeployed without the port-forward connection.

The following example demonstrates use of the ``oc port-forward _`RESOURCE`_ _`EXTERNAL_PORT:CONTAINER_PORT`_`` command to listen locally on port `8080` and to forward connections to port `80` on the pod:

[user@host ~]$ **`oc port-forward nginx-app-cc78k 8080:80`**
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80

### Connect to Running Containers

Administrators use CLI tools to connect to a container via a shell for forensic inspections. With this approach, you can connect to, inspect, and run any available commands within the specified container.

The following example demonstrates use of the ``oc rsh _`POD_NAME`_`` command to connect to a container via a shell:

[user@host ~]$ **`oc rsh tomcat-app-jw53r`**
sh-4.4#

If you need to connect to a specific container in a pod, then use the ``-c _`container_name`_`` option to specify the container name. If you omit this option, then the command connects to the first container in the pod.

You can also connect to running containers from the web console by clicking the Terminal tab in the pod's principal menu.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/troubleshooting/assets/podterminalmenu.png)|

If you have more than one container, then you can change between them to connect to the CLI.

|   |
|---|
|![](https://static.ole.redhat.com/rhls/courses/do180-4.14/images/pods/troubleshooting/assets/containerterminal.png)|

### Execute Commands in a Container

Passing commands to execute within a container from the CLI is another method for troubleshooting a running container. Use this method to send a command to run within the container, or to connect to the container, when further investigation is necessary.

Use the following command to pass and execute commands in a container:

**``oc exec _`POD | TYPE/NAME`_ [-c _`container_name`_] -- _`COMMAND`_ [_`arg1`_ ... _`argN`_]``**

If you omit the ``-c _`container_name`_`` option, then the command targets the first container in the pod.

The following examples demonstrate the use of the `oc exec` command to execute the `ls` command in a container to list the contents of the container's root directory:

[user@host ~]$ **`oc exec -it mariadb-lc78h -- ls /`**
bin  boot  dev	etc  help.1  home  lib	lib64 ...
_...output omitted..._

[user@host ~]$ **`oc exec mariadb-lc78h -- ls /`**
bin
boot
dev
etc
_...output omitted..._

### Note

It is common to add the `-it` flags to the `kubectl exec` or `oc exec` commands. These flags instruct the command to send `STDIN` to the container and `STDOUT`/`STDERR` back to the terminal. The format of the command output is impacted by the inclusion of the `-it` flags.

### Container Events and Logs

Reviewing the historical actions for a container can offer insights into both the lifecycle and health of the deployment. Retrieving the cluster logs provides the chronological details of the container actions. Administrators inspect this log output for information and issues that occur in the running container.

For the following commands, use the ``-c _`container_name`_`` to specify a container in the pod. If you omit this option, then the command targets the first container in the pod.

The following examples demonstrate use of the ``oc logs _`POD_NAME`_`` command to retrieve the logs for a pod:

[user@host ~]$ **`oc logs BIND9-app-rw43j`**
Defaulted container "dns" out of: dns
.:5353
[INFO] plugin/reload: Running configuration SHA512 = 7c3d...3587
CoreDNS-1.9.2
_...output omitted..._

In Kubernetes, an `event` resource is a report of an event somewhere in the cluster. You can use the `kubectl get events` and `oc get events` commands to view pod events in a namespace:

[user@host ~]$ **`oc get events`**
LAST SEEN   TYPE     REASON           OBJECT                           MESSAGE
_...output omitted..._
21m         Normal   AddedInterface   pod/php-app-5d9b84b588-kzfxd     Add eth0 [10.8.0.93/23] from ovn-kubernetes
21m         Normal   Pulled           pod/php-app-5d9b84b588-kzfxd     Container image "registry.ocp4.example.com:8443/redhattraining/php-webapp:v4" already present on machine
21m         Normal   Created          pod/php-app-5d9b84b588-kzfxd     Created container php-webapp
21m         Normal   Started          pod/php-app-5d9b84b588-kzfxd     Started container php-webapp

### Available Linux Commands in Containers

The use of Linux commands for troubleshooting applications can also help with troubleshooting containers. However, when connecting to a container, only the defined tools and applications within the container are available. You can augment the environment inside the container by adding the tools from this section or any other remedial tools to the container image.

Before you add tools to a container image, consider how the tools affect your container image.

- Additional tools increase the size of the image, which might impact container performance.
    
- Tools might require additional update packages and licensing terms, which can impact the ease of updating and distributing the container image.
    
- Hackers might exploit tools in the image.
    

### Troubleshooting from Inside the Cluster

It is routine to troubleshoot a cluster, its components, or the running applications by connecting remotely. This approach assumes that the administrator's computer contains the necessary tools for the work. When unanticipated issues arise, the necessary tools might not be available from an administrator's computer or an alternative machine.

Administrators can alternatively author and deploy a container within the cluster for investigation and remediation. By creating a container image that includes the cluster troubleshooting tools, you have a reliable environment to perform these tasks from any computer with access to the cluster. This approach ensures that an administrator always has access to the tools for reliable troubleshooting and remediation of issues.

Additionally, administrators should plan to author a container image that provides the most valuable troubleshooting tools for containerized applications. In this way, you deploy this "toolbox" container to supplement the forensic process and to provide an environment with the required commands and tools for troubleshooting problematic containers. For example, the "toolbox" container can test how resources operate inside a cluster, such as to confirm whether a pod can connect to resources outside the cluster. Regular cluster users can also create a "toolbox" container to help with application troubleshooting. For example, a regular user could run a pod with a MySQL client to connect to another pod that runs a MySQL server.

Although this approach falls outside the focus of this course, because it is more application-level remediation than container-level troubleshooting, it is important to realize that containers have such capacity.